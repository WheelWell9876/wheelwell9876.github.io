{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWFnjTT-zc3p"
      },
      "source": [
        "# **Import and Set Paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3b6cMRO7DVk"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas\n",
        "!pip install shapely\n",
        "!pip install geofeather"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bf2XzRbzc3u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "from io import StringIO\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\", module=\"fiona.ogrext\")\n",
        "\n",
        "# Create a custom stream class to filter out specific messages\n",
        "class FilteredStream:\n",
        "    def __init__(self, stream):\n",
        "        self.stream = stream\n",
        "        self.buffer = StringIO()\n",
        "\n",
        "    def write(self, data):\n",
        "        if \"WARNING:fiona.ogrext:Expecting property name enclosed in double quotes\" not in data:\n",
        "            self.stream.write(data)\n",
        "        else:\n",
        "            self.buffer.write(data)  # Capture discarded data if needed\n",
        "\n",
        "    def flush(self):\n",
        "        self.stream.flush()\n",
        "\n",
        "# Replace sys.stderr with our filtered stream\n",
        "sys.stderr = FilteredStream(sys.stderr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0oLRRxOzc33"
      },
      "source": [
        "# **Weights**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BPO7zU7zc35"
      },
      "source": [
        "## Communications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0y0XEDozc35"
      },
      "source": [
        "### Cell Towers\n",
        "\n",
        "**Dataset: Cellular Towers** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::cellular-towers-1/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWWy6n5rzc36",
        "outputId": "9d1fe3ee-c85a-4481-bc30-c05701b846e0"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Define the weights for StrucType\n",
        "struc_type_weights = {\n",
        "    'GTOWER': 0.100,\n",
        "    'LTOWER': 0.110,\n",
        "    'MAST': 0.050,\n",
        "    'POLE': 0.040,\n",
        "    'MTOWER': 0.060,\n",
        "    'TANK': 0.030,\n",
        "    'B': 0.020,\n",
        "    'BTWR': 0.005,\n",
        "    'UPOLE': 0.010,\n",
        "    'TOWER': 0.160,\n",
        "    'Other': 0.345,  # Represents the sum of other structure types\n",
        "    'Blank': 0.070\n",
        "}\n",
        "\n",
        "# Define the weight for LicStatus\n",
        "lic_status_weights = {\n",
        "    'A': 0.500,  # Active\n",
        "    'Inactive': 0.000  # Assuming that non-active statuses are not as important\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Function to calculate the weight for each row\n",
        "def calculate_tower_weight(row):\n",
        "    # Get the weight for the structure type\n",
        "    struc_type_weight = struc_type_weights.get(row['StrucType'], struc_type_weights['Other'])\n",
        "\n",
        "    # Get the weight for the license status\n",
        "    lic_status_weight = lic_status_weights.get(row['LicStatus'], 0.000)\n",
        "\n",
        "    # Calculate the combined weight\n",
        "    combined_weight = 0.5 * struc_type_weight + 0.5 * lic_status_weight\n",
        "\n",
        "    # Ensure the weight is valid (non-negative and not null)\n",
        "    return ensure_valid_weight(combined_weight)\n",
        "\n",
        "# Function to apply weights and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation and field stripping...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply weight calculation\n",
        "    gdf['Weight'] = gdf.apply(calculate_tower_weight, axis=1)\n",
        "\n",
        "    # Strip unnecessary fields, keeping only essential ones: Weight and geometry\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Function to process, save, and display file size reduction\n",
        "def process_and_save_stripped_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_stripped = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the stripped and weighted dataset to the new folder\n",
        "    print(f\"Saving stripped and weighted dataset to {output_path}...\")\n",
        "    gdf_stripped.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define input and output paths\n",
        "input_path = \"/geoJSON/cleaned/Cellular_Towers_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_stripped_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lImJw3I-zc36"
      },
      "source": [
        "### Microwave Service Towers\n",
        "\n",
        "**Dataset: Microwave Service Towers** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::microwave-service-towers/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGZHMDvR6ljb",
        "outputId": "093b0b1a-2e6a-4592-f6ad-80ddffd26415"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAFxORNVzc36",
        "outputId": "7bc0d0cd-8a5b-4d1a-8846-9bf7e4c4e9df"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Define the weights for StrucType\n",
        "struc_type_weights = {\n",
        "    'GTOWER': 0.100,\n",
        "    'LTOWER': 0.150,\n",
        "    'TOWER': 0.070,\n",
        "    'MTOWER': 0.060,\n",
        "    'TANK': 0.040,\n",
        "    'MAST': 0.040,\n",
        "    'POLE': 0.040,\n",
        "    'BANT': 0.030,\n",
        "    'SILO': 0.020,\n",
        "    'Other': 0.350,  # Represents the sum of other structure types\n",
        "    'Blank': 0.000   # Assuming blank or undefined entries have no importance\n",
        "}\n",
        "\n",
        "# Define the weight for LicStatus\n",
        "lic_status_weights = {\n",
        "    'A': 0.500,  # Active\n",
        "    'Inactive': 0.000  # Assuming that non-active statuses are not as important\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Function to calculate the weight for each row\n",
        "def calculate_tower_weight(row):\n",
        "    # Get the weight for the structure type\n",
        "    struc_type_weight = struc_type_weights.get(row['StrucType'], struc_type_weights['Other'])\n",
        "\n",
        "    # Get the weight for the license status\n",
        "    lic_status_weight = lic_status_weights.get(row['LicStatus'], 0.000)\n",
        "\n",
        "    # Calculate the combined weight\n",
        "    combined_weight = (\n",
        "        0.5 * struc_type_weight +  # 50% weight to structure type\n",
        "        0.5 * lic_status_weight    # 50% weight to license status\n",
        "    )\n",
        "\n",
        "    # Ensure the weight is valid (non-negative and not null)\n",
        "    return ensure_valid_weight(combined_weight)\n",
        "\n",
        "# Function to apply weights and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation and field stripping...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply weight calculation\n",
        "    gdf['Weight'] = gdf.apply(calculate_tower_weight, axis=1)\n",
        "\n",
        "    # Strip unnecessary fields, keeping only essential ones: Weight and geometry\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Function to process, save, and display file size reduction\n",
        "def process_and_save_stripped_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_stripped = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the stripped and weighted dataset to the new folder\n",
        "    print(f\"Saving stripped and weighted dataset to {output_path}...\")\n",
        "    gdf_stripped.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define input and output paths\n",
        "input_path = \"/geoJSON/cleaned/Microwave_Service_Towers_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_stripped_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7CqMViUzc37"
      },
      "source": [
        "## Education"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxrD-ojmzc37"
      },
      "source": [
        "### Colleges and Universities\n",
        "**Dataset: Colleges and Universities -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::colleges-and-universities/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F49HGwafzc37",
        "outputId": "5a82d38f-882e-4b12-f9a3-3623909d8f9a"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    3: 0.150,\n",
        "    1: 0.300,\n",
        "    2: 0.200,\n",
        "    -3: 0.000\n",
        "}\n",
        "\n",
        "sector_weights = {\n",
        "    2: 0.200,\n",
        "    9: 0.150,\n",
        "    4: 0.150,\n",
        "    1: 0.100,\n",
        "    'Other': 0.160\n",
        "}\n",
        "\n",
        "level_weights = {\n",
        "    1: 0.300,\n",
        "    3: 0.250,\n",
        "    2: 0.200,\n",
        "    -3: 0.000\n",
        "}\n",
        "\n",
        "hi_offer_weights = {\n",
        "    0: 0.050,\n",
        "    40: 0.300,\n",
        "    20: 0.200,\n",
        "    30: 0.150,\n",
        "    'Other': 0.280\n",
        "}\n",
        "\n",
        "deg_grant_weights = {\n",
        "    1: 0.400,\n",
        "    2: 0.100,\n",
        "    -3: 0.000\n",
        "}\n",
        "\n",
        "inst_size_weights = {\n",
        "    1: 0.200,\n",
        "    2: 0.150,\n",
        "    3: 0.100,\n",
        "    4: 0.100,\n",
        "    'Other': 0.050\n",
        "}\n",
        "\n",
        "size_set_weights = {\n",
        "    -2: 0.050,\n",
        "    18: 0.050,\n",
        "    22: 0.050,\n",
        "    'Other': 0.050\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.650,\n",
        "    'SECTOR': 0.760,\n",
        "    'LEVEL_': 0.750,\n",
        "    'HI_OFFER': 0.980,\n",
        "    'DEG_GRANT': 0.500,\n",
        "    'INST_SIZE': 0.600,\n",
        "    'SIZE_SET': 0.200\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'POPULATION': 0.150,\n",
        "    'PT_ENROLL': 0.050,\n",
        "    'FT_ENROLL': 0.150,\n",
        "    'TOT_ENROLL': 0.150\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_college_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = ensure_valid_weight(type_weights.get(row.get('TYPE', 0), 0.0)) * qualitative_importance_grades['TYPE']\n",
        "    sector_weight = ensure_valid_weight(sector_weights.get(row.get('SECTOR', 'Other'), sector_weights['Other'])) * qualitative_importance_grades['SECTOR']\n",
        "    level_weight = ensure_valid_weight(level_weights.get(row.get('LEVEL_', 0), 0.0)) * qualitative_importance_grades['LEVEL_']\n",
        "    hi_offer_weight = ensure_valid_weight(hi_offer_weights.get(row.get('HI_OFFER', 'Other'), hi_offer_weights['Other'])) * qualitative_importance_grades['HI_OFFER']\n",
        "    deg_grant_weight = ensure_valid_weight(deg_grant_weights.get(row.get('DEG_GRANT', 0), 0.0)) * qualitative_importance_grades['DEG_GRANT']\n",
        "    inst_size_weight = ensure_valid_weight(inst_size_weights.get(row.get('INST_SIZE', 'Other'), inst_size_weights['Other'])) * qualitative_importance_grades['INST_SIZE']\n",
        "    size_set_weight = ensure_valid_weight(size_set_weights.get(row.get('SIZE_SET', 'Other'), size_set_weights['Other'])) * qualitative_importance_grades['SIZE_SET']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    pop_weight = ensure_valid_weight((row.get('POPULATION', 0) / np.nanmax([row.get('POPULATION', 1), 1])) * quantitative_weights['POPULATION']) if not pd.isna(row.get('POPULATION')) else 0\n",
        "    pt_enroll_weight = ensure_valid_weight((row.get('PT_ENROLL', 0) / np.nanmax([row.get('PT_ENROLL', 1), 1])) * quantitative_weights['PT_ENROLL']) if not pd.isna(row.get('PT_ENROLL')) else 0\n",
        "    ft_enroll_weight = ensure_valid_weight((row.get('FT_ENROLL', 0) / np.nanmax([row.get('FT_ENROLL', 1), 1])) * quantitative_weights['FT_ENROLL']) if not pd.isna(row.get('FT_ENROLL')) else 0\n",
        "    tot_enroll_weight = ensure_valid_weight((row.get('TOT_ENROLL', 0) / np.nanmax([row.get('TOT_ENROLL', 1), 1])) * quantitative_weights['TOT_ENROLL']) if not pd.isna(row.get('TOT_ENROLL')) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        type_weight +\n",
        "        sector_weight +\n",
        "        level_weight +\n",
        "        hi_offer_weight +\n",
        "        deg_grant_weight +\n",
        "        inst_size_weight +\n",
        "        size_set_weight +\n",
        "        pop_weight +\n",
        "        pt_enroll_weight +\n",
        "        ft_enroll_weight +\n",
        "        tot_enroll_weight\n",
        "    )\n",
        "\n",
        "    return ensure_valid_weight(total_weight)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation and field stripping...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_college_weight, axis=1)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Colleges_and_Universities_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jInla1rzc37"
      },
      "source": [
        "### Private Schools\n",
        "**Dataset: Private Schools -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::private-schools/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAMDEgPVzc38",
        "outputId": "e0954899-4aba-4827-deaf-0d8f6e461c57"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    1: 0.500,\n",
        "    2: 0.150,\n",
        "    7: 0.120,\n",
        "    4: 0.100,\n",
        "    6: 0.070,\n",
        "    3: 0.050,\n",
        "    5: 0.010\n",
        "}\n",
        "\n",
        "level_weights = {\n",
        "    1: 0.600,\n",
        "    3: 0.300,\n",
        "    2: 0.100\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.500,\n",
        "    'LEVEL_': 0.500\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'POPULATION': 0.250,\n",
        "    'ENROLLMENT': 0.200,\n",
        "    'FT_TEACHERS': 0.050\n",
        "}\n",
        "\n",
        "# Ensure weight is valid (non-null, non-negative)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set invalid weights to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure positive value\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_school_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = ensure_valid_weight(type_weights.get(row.get('TYPE', 0), 0.0)) * qualitative_importance_grades['TYPE']\n",
        "    level_weight = ensure_valid_weight(level_weights.get(row.get('LEVEL_', 0), 0.0)) * qualitative_importance_grades['LEVEL_']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    pop_weight = ensure_valid_weight((row.get('POPULATION', 0) / np.nanmax([row.get('POPULATION', 1), 1])) * quantitative_weights['POPULATION']) if not pd.isna(row.get('POPULATION')) else 0\n",
        "    enroll_weight = ensure_valid_weight((row.get('ENROLLMENT', 0) / np.nanmax([row.get('ENROLLMENT', 1), 1])) * quantitative_weights['ENROLLMENT']) if not pd.isna(row.get('ENROLLMENT')) else 0\n",
        "    ft_teachers_weight = ensure_valid_weight((row.get('FT_TEACHERS', 0) / np.nanmax([row.get('FT_TEACHERS', 1), 1])) * quantitative_weights['FT_TEACHERS']) if not pd.isna(row.get('FT_TEACHERS')) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        type_weight +\n",
        "        level_weight +\n",
        "        pop_weight +\n",
        "        enroll_weight +\n",
        "        ft_teachers_weight\n",
        "    )\n",
        "\n",
        "    return ensure_valid_weight(total_weight)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation for each school...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_school_weight, axis=1)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Private_Schools_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuMue-Bvzc3-"
      },
      "source": [
        "## Emergency Services"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rKvqw2Czc3-"
      },
      "source": [
        "### Fire and Emergency Services\n",
        "**Dataset: Fire and Emergency Medical Service (EMS) Stations -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::fire-and-emergency-medical-service-ems-stations/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av-T9g1xzc3-",
        "outputId": "ea77f340-4078-4d58-f629-f72561399d27"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "distribution_policy_weights = {\n",
        "    'E4': 0.150\n",
        "}\n",
        "\n",
        "ftype_weights = {\n",
        "    740: 0.200\n",
        "}\n",
        "\n",
        "data_security_weights = {\n",
        "    5.0: 0.250\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'DISTRIBUTION_POLICY': 0.150,\n",
        "    'FTYPE': 0.200,\n",
        "    'DATA_SECURITY': 0.250\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_ems_station_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    distribution_policy_weight = ensure_valid_weight(distribution_policy_weights.get(row.get('DISTRIBUTION_POLICY', 0), 0.0)) * qualitative_importance_grades['DISTRIBUTION_POLICY']\n",
        "    ftype_weight = ensure_valid_weight(ftype_weights.get(row.get('FTYPE', 0), 0.0)) * qualitative_importance_grades['FTYPE']\n",
        "    data_security_weight = ensure_valid_weight(data_security_weights.get(row.get('DATA_SECURITY', 0.0), 0.0)) * qualitative_importance_grades['DATA_SECURITY']\n",
        "\n",
        "    # Combine qualitative weights (no quantitative attributes in this dataset)\n",
        "    total_weight = (\n",
        "        distribution_policy_weight +\n",
        "        ftype_weight +\n",
        "        data_security_weight\n",
        "    )\n",
        "\n",
        "    return ensure_valid_weight(total_weight)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation for each EMS station...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_ems_station_weight, axis=1)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/EMS_Stations_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtIvvgTazc3_"
      },
      "source": [
        "### Local Law Enforcement\n",
        "**Dataset: Local Law Enforcement Locations -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::local-law-enforcement-locations/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10TBq8vUzc3_",
        "outputId": "6ec83ac8-36e3-41d7-f7d0-18e77e343507"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    \"LOCAL POLICE DEPARTMENT\": 0.400,\n",
        "    \"SHERIFF'S OFFICE\": 0.250,\n",
        "    \"SPECIAL JURISDICTION\": 0.150,\n",
        "    \"PRIMARY STATE AGENCY\": 0.150,\n",
        "    \"CONSTABLE/MARSHAL\": 0.050\n",
        "}\n",
        "\n",
        "status_weights = {\n",
        "    \"OPEN\": 0.950,\n",
        "    \"NOT AVAILABLE\": 0.050\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.650,\n",
        "    'STATUS': 0.350\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'POPULATION': 0.200,\n",
        "    'FTSWORN': 0.400,\n",
        "    'FTCIV': 0.200,\n",
        "    'PTSWORN': 0.100,\n",
        "    'PTCIV': 0.100\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_law_enforcement_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = ensure_valid_weight(type_weights.get(row.get('TYPE', 0), 0.0)) * qualitative_importance_grades['TYPE']\n",
        "    status_weight = ensure_valid_weight(status_weights.get(row.get('STATUS', 0), 0.0)) * qualitative_importance_grades['STATUS']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    pop_weight = ensure_valid_weight((row.get('POPULATION', 0) / np.nanmax([row.get('POPULATION', 1), 1])) * quantitative_weights['POPULATION']) if not pd.isna(row.get('POPULATION')) else 0\n",
        "    ftsworn_weight = ensure_valid_weight((row.get('FTSWORN', 0) / np.nanmax([row.get('FTSWORN', 1), 1])) * quantitative_weights['FTSWORN']) if not pd.isna(row.get('FTSWORN')) else 0\n",
        "    ftciv_weight = ensure_valid_weight((row.get('FTCIV', 0) / np.nanmax([row.get('FTCIV', 1), 1])) * quantitative_weights['FTCIV']) if not pd.isna(row.get('FTCIV')) else 0\n",
        "    ptsworn_weight = ensure_valid_weight((row.get('PTSWORN', 0) / np.nanmax([row.get('PTSWORN', 1), 1])) * quantitative_weights['PTSWORN']) if not pd.isna(row.get('PTSWORN')) else 0\n",
        "    ptciv_weight = ensure_valid_weight((row.get('PTCIV', 0) / np.nanmax([row.get('PTCIV', 1), 1])) * quantitative_weights['PTCIV']) if not pd.isna(row.get('PTCIV')) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        type_weight +\n",
        "        status_weight +\n",
        "        pop_weight +\n",
        "        ftsworn_weight +\n",
        "        ftciv_weight +\n",
        "        ptsworn_weight +\n",
        "        ptciv_weight\n",
        "    )\n",
        "\n",
        "    return ensure_valid_weight(total_weight)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation for each law enforcement agency...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_law_enforcement_weight, axis=1)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Local_Law_Enforcement_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa5rQgEBzc4A"
      },
      "source": [
        "## Energy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmIn1z6mzc4A"
      },
      "source": [
        "### Electric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VBrXiHDzc4A"
      },
      "source": [
        "#### Electric Power Transformers and Grids\n",
        "**Dataset: Electric Power Transmission Lines -** https://hifld-geoplatform.opendata.arcgis.com/datasets/geoplatform::electric-power-transmission-lines/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gymwhRQzc4B",
        "outputId": "cf390118-3d37-41e5-dac2-69f5431db014"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    'AC; OVERHEAD': 0.500,\n",
        "    'OVERHEAD': 0.200,\n",
        "    'AC': 0.100,\n",
        "    'NOT AVAILABLE': 0.000,\n",
        "    'AC; UNDERGROUND': 0.050,\n",
        "    'UNDERGROUND': 0.030,\n",
        "    'DC; OVERHEAD': 0.010,\n",
        "    'DC': 0.005,\n",
        "    'DC; UNDERGROUND': 0.005\n",
        "}\n",
        "\n",
        "status_weights = {\n",
        "    'IN SERVICE': 0.800,\n",
        "    'NOT AVAILABLE': 0.000,\n",
        "    'INACTIVE': 0.050,\n",
        "    'UNDER CONSTRUCTION': 0.030,\n",
        "    'PROPOSED': 0.020\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.900,\n",
        "    'STATUS': 0.900\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'VOLTAGE': 0.200\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_power_grid_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = ensure_valid_weight(type_weights.get(row.get('TYPE', 0), 0.0)) * qualitative_importance_grades['TYPE']\n",
        "    status_weight = ensure_valid_weight(status_weights.get(row.get('STATUS', 0), 0.0)) * qualitative_importance_grades['STATUS']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    voltage_weight = ensure_valid_weight((row.get('VOLTAGE', 0) / np.nanmax([row.get('VOLTAGE', 1), 1])) * quantitative_weights['VOLTAGE']) if not pd.isna(row.get('VOLTAGE')) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        type_weight +\n",
        "        status_weight +\n",
        "        voltage_weight\n",
        "    )\n",
        "\n",
        "    return ensure_valid_weight(total_weight)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation for each power transmission line...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_power_grid_weight, axis=1)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Power_Transmission_Lines_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nXxDht9zc4D"
      },
      "source": [
        "#### Peak Shaving Facilities\n",
        "**Dataset: Peak Shaving Facilities -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::peak-shaving-facilities/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkJqUkYSzc4E",
        "outputId": "7dd25501-67ec-4993-863b-b6c33c42b09d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from /content/drive/MyDrive/CI_Sentinal/geoJSON/cleaned/Peak_Shaving_Facilities_Cleaned.geojson...\n",
            "Dataset loaded successfully.\n",
            "Initial file size: 0.03 MB\n",
            "Starting weight calculation for each peak shaving facility...\n",
            "Weight calculation and field stripping completed in 0.12 seconds.\n",
            "Saving weighted dataset to /content/drive/MyDrive/CI_Sentinal/geoJSON/cleaned_weighted/Peak_Shaving_Facilities_Cleaned_weighted_cleaned.geojson...\n",
            "Final file size: 0.01 MB\n",
            "File size reduced by: 0.02 MB (59.92% reduction).\n"
          ]
        }
      ],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    'LNG PEAK SHAVING': 0.800,\n",
        "    'PROPANE AIR PEAK SHAVING': 0.200\n",
        "}\n",
        "\n",
        "status_weights = {\n",
        "    'ACTIVE': 0.900,\n",
        "    'UNDER CONSTRUCTION': 0.050,\n",
        "    'UNKNOWN': 0.050\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.500,\n",
        "    'STATUS': 0.500\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'NUMTANKS': 0.200,\n",
        "    'NUM_VAPE': 0.150,\n",
        "    'MAXVAPCAP': 0.250,\n",
        "    'TOTALCAP': 0.400\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_peak_shaving_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = ensure_valid_weight(type_weights.get(row.get('TYPE', 0), 0.0)) * qualitative_importance_grades['TYPE']\n",
        "    status_weight = ensure_valid_weight(status_weights.get(row.get('STATUS', 0), 0.0)) * qualitative_importance_grades['STATUS']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    numtanks_weight = ensure_valid_weight((row.get('NUMTANKS', 0) / np.nanmax([row.get('NUMTANKS', 1), 1])) * quantitative_weights['NUMTANKS']) if not pd.isna(row.get('NUMTANKS')) else 0\n",
        "    num_vape_weight = ensure_valid_weight((row.get('NUM_VAPE', 0) / np.nanmax([row.get('NUM_VAPE', 1), 1])) * quantitative_weights['NUM_VAPE']) if not pd.isna(row.get('NUM_VAPE')) else 0\n",
        "    maxvapcap_weight = ensure_valid_weight((row.get('MAXVAPCAP', 0) / np.nanmax([row.get('MAXVAPCAP', 1), 1])) * quantitative_weights['MAXVAPCAP']) if not pd.isna(row.get('MAXVAPCAP')) else 0\n",
        "    totalcap_weight = ensure_valid_weight((row.get('TOTALCAP', 0) / np.nanmax([row.get('TOTALCAP', 1), 1])) * quantitative_weights['TOTALCAP']) if not pd.isna(row.get('TOTALCAP')) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        type_weight +\n",
        "        status_weight +\n",
        "        numtanks_weight +\n",
        "        num_vape_weight +\n",
        "        maxvapcap_weight +\n",
        "        totalcap_weight\n",
        "    )\n",
        "\n",
        "    return ensure_valid_weight(total_weight)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation for each peak shaving facility...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_peak_shaving_weight, axis=1)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Peak_Shaving_Facilities_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5_OEbZjzc4E"
      },
      "source": [
        "### Gas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMJiDDZczc4E"
      },
      "source": [
        "#### Adove Ground LNG Storage\n",
        "**Dataset: Above Ground LNG Storage Facilities -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::above-ground-lng-storage-facilities/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hYgRDzazc4M"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    'VEHICULAR FUEL': 0.400,\n",
        "    'STORAGE W LIQUEFACTION': 0.300,\n",
        "    'STORAGE W/O LIQUEFACTION': 0.200,\n",
        "    'STORAGE W BOTH': 0.050,\n",
        "    'STRANDED UTILITY': 0.015,\n",
        "    'NOT AVAILABLE': 0.015,\n",
        "    'OTHER': 0.010,\n",
        "    'STORAGE - BUNKERING': 0.010\n",
        "}\n",
        "\n",
        "status_weights = {\n",
        "    'ACTIVE': 0.950,\n",
        "    'UNDER CONSTRUCTION': 0.020,\n",
        "    'ABANDONED': 0.015,\n",
        "    'NOT AVAILABLE': 0.015\n",
        "}\n",
        "\n",
        "lng_source_weights = {\n",
        "    'TRUCK': 0.700,\n",
        "    'LIQUEFACTION': 0.290,\n",
        "    'NOT AVAILABLE': 0.010\n",
        "}\n",
        "\n",
        "con_type_weights = {\n",
        "    'VERTICAL TANK': 0.400,\n",
        "    'REFRIGERATED TANK': 0.300,\n",
        "    'ISO HORIZONTAL TANK': 0.250,\n",
        "    'NOT AVAILABLE': 0.030,\n",
        "    'VERTICAL TANK / ISO HORIZONTAL TANK': 0.020\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.300,\n",
        "    'STATUS': 0.300,\n",
        "    'LNG_SOURCE': 0.250,\n",
        "    'CON_TYPE': 0.150\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'NUMTANKS': 0.300,\n",
        "    'TOTALCAP': 0.700\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_lng_storage_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = ensure_valid_weight(type_weights.get(row.get('TYPE', 0), 0.0)) * qualitative_importance_grades['TYPE']\n",
        "    status_weight = ensure_valid_weight(status_weights.get(row.get('STATUS', 0), 0.0)) * qualitative_importance_grades['STATUS']\n",
        "    lng_source_weight = ensure_valid_weight(lng_source_weights.get(row.get('LNG_SOURCE', 0), 0.0)) * qualitative_importance_grades['LNG_SOURCE']\n",
        "    con_type_weight = ensure_valid_weight(con_type_weights.get(row.get('CON_TYPE', 0), 0.0)) * qualitative_importance_grades['CON_TYPE']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    numtanks_weight = ensure_valid_weight((row.get('NUMTANKS', 0) / np.nanmax([row.get('NUMTANKS', 1), 1])) * quantitative_weights['NUMTANKS']) if not pd.isna(row.get('NUMTANKS')) else 0\n",
        "    totalcap_weight = ensure_valid_weight((row.get('TOTALCAP', 0) / np.nanmax([row.get('TOTALCAP', 1), 1])) * quantitative_weights['TOTALCAP']) if not pd.isna(row.get('TOTALCAP')) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        type_weight +\n",
        "        status_weight +\n",
        "        lng_source_weight +\n",
        "        con_type_weight +\n",
        "        numtanks_weight +\n",
        "        totalcap_weight\n",
        "    )\n",
        "\n",
        "    return ensure_valid_weight(total_weight)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation for each LNG storage facility...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize the progress bar\n",
        "    with tqdm(total=len(gdf), desc=\"Calculating weights\", unit=\"rows\") as pbar:\n",
        "        for index, row in gdf.iterrows():\n",
        "            gdf.at[index, 'Weight'] = calculate_lng_storage_weight(row)\n",
        "            if index % 100 == 0:\n",
        "                pbar.update(100)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/LNG_Storage_Facilities_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0qruHpWzc4M"
      },
      "source": [
        "#### Biodiesel Plants\n",
        "**Dataset: Biodiesel Plants -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::biodiesel-plants/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoCtCdOozc4M",
        "outputId": "71d8c735-fcb2-4d54-80ee-7d70e526f382"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schema for Cap_Mmgal\n",
        "cap_mmgal_weight = 1.000  # Cap_Mmgal is the key attribute here\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set invalid or missing weights to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure positive weights\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_biodiesel_plant_weight(row):\n",
        "    # Calculate weight based on Cap_Mmgal\n",
        "    cap_weight = ensure_valid_weight((row.get('Cap_Mmgal', 0) / np.nanmax([row.get('Cap_Mmgal', 1), 1])) * cap_mmgal_weight) if not pd.isna(row.get('Cap_Mmgal')) else 0\n",
        "\n",
        "    # Since we only have Cap_Mmgal, the total weight is the cap_weight\n",
        "    total_weight = cap_weight\n",
        "\n",
        "    return ensure_valid_weight(total_weight)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation for each biodiesel plant...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_biodiesel_plant_weight, axis=1)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Biodiesel_Plants_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C6RP7OSzc4N"
      },
      "source": [
        "#### Ethanol Plants\n",
        "**Dataset: Ethanol Plants -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::ethanol-plants/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHTIugNkzc4N",
        "outputId": "f010b17c-d94b-4387-f1f3-46462e0b0f0d"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    'ETHANOL PLANT': 1.000\n",
        "}\n",
        "\n",
        "status_weights = {\n",
        "    'IN SERVICE': 0.950,\n",
        "    'UNDER CONSTRUCTION': 0.040,\n",
        "    'IDLE': 0.010\n",
        "}\n",
        "\n",
        "feedstock_weights = {\n",
        "    'CORN KERNELS, MILO, GRAINS, SUGAR': 0.300,\n",
        "    'CORN KERNELS': 0.200,\n",
        "    'CORN KERNELS, MILO': 0.050,\n",
        "    'GRAINS': 0.050,\n",
        "    'BEVERAGE AND/OR BREWERY WASTE': 0.030,\n",
        "    'MUNICIPAL SOLID WASTE': 0.025,\n",
        "    'CORN KERNELS, GRAINS': 0.025,\n",
        "    'WHEY': 0.020,\n",
        "    'SUGAR': 0.020,\n",
        "    'WOODY BIOMASS': 0.020,\n",
        "    'CORN STOVER': 0.020,\n",
        "    'CORN STOVER, ENERGY GRASSES': 0.020,\n",
        "    'CORN KERNELS, SUGAR': 0.020,\n",
        "    'WASTE DRIVEN': 0.010,\n",
        "    'WOODY BIOMASS, WOODY SUGARS': 0.010,\n",
        "    'CORN KERNELS, CORN KERNEL FIBER': 0.010,\n",
        "    'MILO, GRAINS': 0.010,\n",
        "    'ENERGY GRASSES': 0.010,\n",
        "    'WOOD SUGARS': 0.010,\n",
        "    'CORN KERNELS, MILO, GRAINS': 0.010,\n",
        "    'CORN KERNELS, ENERGY TOBACCO': 0.010,\n",
        "    'WASTE SUGAR, STARCHES': 0.010,\n",
        "    'CORN STOVER, ENERGY GRASSES, WOODY BIOMASS': 0.010,\n",
        "    'MUNICIPAL SOLID WASTE, WOODY BIOMASS': 0.010\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.300,\n",
        "    'STATUS': 0.400,\n",
        "    'FEEDSTOCK': 0.300\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'CURRENTCAP': 0.700,\n",
        "    'NAMEPLATE': 0.300\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_ethanol_plant_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = ensure_valid_weight(type_weights.get(row.get('TYPE', 0), 0.0)) * qualitative_importance_grades['TYPE']\n",
        "    status_weight = ensure_valid_weight(status_weights.get(row.get('STATUS', 0), 0.0)) * qualitative_importance_grades['STATUS']\n",
        "    feedstock_weight = ensure_valid_weight(feedstock_weights.get(row.get('FEEDSTOCK', 0), 0.0)) * qualitative_importance_grades['FEEDSTOCK']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    currentcap_weight = ensure_valid_weight((row.get('CURRENTCAP', 0) / np.nanmax([row.get('CURRENTCAP', 1), 1])) * quantitative_weights['CURRENTCAP']) if not pd.isna(row.get('CURRENTCAP')) else 0\n",
        "    nameplate_weight = ensure_valid_weight((row.get('NAMEPLATE', 0) / np.nanmax([row.get('NAMEPLATE', 1), 1])) * quantitative_weights['NAMEPLATE']) if not pd.isna(row.get('NAMEPLATE')) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        type_weight +\n",
        "        status_weight +\n",
        "        feedstock_weight +\n",
        "        currentcap_weight +\n",
        "        nameplate_weight\n",
        "    )\n",
        "\n",
        "    # Round the total weight to 4 decimal places\n",
        "    return round(ensure_valid_weight(total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove unnecessary fields\n",
        "\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation for each ethanol plant...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_ethanol_plant_weight, axis=1)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Ethanol_Plants_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmtltYKszc4N"
      },
      "source": [
        "#### Hydrocarbon Gas Liquid Pipelines\n",
        "**Dataset: Hydrocarbon Gas Liquid Pipelines -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::hydrocarbon-gas-liquid-pipelines/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_gY0CNbzc4N",
        "outputId": "d2d82c13-c90f-4699-dfc0-29cb77be58c3"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "qualitative_weights = {\n",
        "    'Shape_Leng': 0.000001,\n",
        "    'Shape__Length': 0.000001\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'Shape_Leng': 0.000001,\n",
        "    'Shape__Length': 0.000001\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights) and round to 4 decimal places\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return round(float(max(weight, 0.0)), 4)  # Ensure the weight is non-negative and rounded to 4 decimal places\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_pipeline_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    shape_leng_weight = row.get('Shape_Leng', 0) * qualitative_importance_grades['Shape_Leng'] if not pd.isna(row.get('Shape_Leng', 0)) else 0\n",
        "    shape_length_weight = row.get('Shape__Length', 0) * qualitative_importance_grades['Shape__Length'] if not pd.isna(row.get('Shape__Length', 0)) else 0\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = (\n",
        "        shape_leng_weight +\n",
        "        shape_length_weight\n",
        "    )\n",
        "\n",
        "    # Return the total weight, ensuring valid weight and rounding to 4 decimal places\n",
        "    return ensure_valid_weight(total_weight)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation for each pipeline segment...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_pipeline_weight, axis=1)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Hydrocarbon_Pipelines_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drDTcYC_zc4N"
      },
      "source": [
        "#### LNG Import and Export Terminals\n",
        "**Dataset: Liquified Natural Gas Import Exports and Terminals -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::liquified-natural-gas-import-exports-and-terminals/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXT9STKwzc4O",
        "outputId": "876651da-e88e-4fc5-d153-330a2d8acf09"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    'IMPORT': 0.600,\n",
        "    'EXPORT': 0.400\n",
        "}\n",
        "\n",
        "status_weights = {\n",
        "    'IN SERVICE': 0.800,\n",
        "    'UNDER CONSTRUCTION': 0.160,\n",
        "    'SUSPENDED': 0.040\n",
        "}\n",
        "\n",
        "contype_weights = {\n",
        "    'LNG TANKER': 0.960,\n",
        "    'ISO CONTAINER': 0.040\n",
        "}\n",
        "\n",
        "ie_port_weights = {\n",
        "    'COVE POINT': 0.120,\n",
        "    'ELBA ISLAND': 0.080,\n",
        "    'HACKBERRY': 0.080,\n",
        "    'SABINE PASS': 0.080,\n",
        "    'FREEPORT': 0.080,\n",
        "    'Other': 0.560  # For all other ports\n",
        "}\n",
        "\n",
        "impexpctry_weights = {\n",
        "    'NOT APPLICABLE': 0.500,\n",
        "    'Country Combinations': 0.500\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.250,\n",
        "    'STATUS': 0.250,\n",
        "    'CONTYPE': 0.250,\n",
        "    'IE_PORT': 0.150,\n",
        "    'IMPEXPCTRY': 0.100\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'STORCAP': 0.200,\n",
        "    'CURRENTCAP': 0.200,\n",
        "    'BERTHS': 0.100,\n",
        "    'STORAGE': 0.200,\n",
        "    'APPCAP': 0.150,\n",
        "    'VOLUME': 0.150\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_lng_terminal_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = ensure_valid_weight(type_weights.get(row.get('TYPE', 0), 0.0)) * qualitative_importance_grades['TYPE']\n",
        "    status_weight = ensure_valid_weight(status_weights.get(row.get('STATUS', 0), 0.0)) * qualitative_importance_grades['STATUS']\n",
        "    contype_weight = ensure_valid_weight(contype_weights.get(row.get('CONTYPE', 0), 0.0)) * qualitative_importance_grades['CONTYPE']\n",
        "    ie_port_weight = ensure_valid_weight(ie_port_weights.get(row.get('IE_PORT', 'Other'), 0.0)) * qualitative_importance_grades['IE_PORT']\n",
        "    impexpctry_weight = ensure_valid_weight(impexpctry_weights.get(row.get('IMPEXPCTRY', 'Country Combinations'), 0.0)) * qualitative_importance_grades['IMPEXPCTRY']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    storcap_weight = ensure_valid_weight((row.get('STORCAP', 0) / np.nanmax([row.get('STORCAP', 1), 1])) * quantitative_weights['STORCAP']) if not pd.isna(row.get('STORCAP')) else 0\n",
        "    currentcap_weight = ensure_valid_weight((row.get('CURRENTCAP', 0) / np.nanmax([row.get('CURRENTCAP', 1), 1])) * quantitative_weights['CURRENTCAP']) if not pd.isna(row.get('CURRENTCAP')) else 0\n",
        "    berths_weight = ensure_valid_weight((row.get('BERTHS', 0) / np.nanmax([row.get('BERTHS', 1), 1])) * quantitative_weights['BERTHS']) if not pd.isna(row.get('BERTHS')) else 0\n",
        "    storage_weight = ensure_valid_weight((row.get('STORAGE', 0) / np.nanmax([row.get('STORAGE', 1), 1])) * quantitative_weights['STORAGE']) if not pd.isna(row.get('STORAGE')) else 0\n",
        "    appcap_weight = ensure_valid_weight((row.get('APPCAP', 0) / np.nanmax([row.get('APPCAP', 1), 1])) * quantitative_weights['APPCAP']) if not pd.isna(row.get('APPCAP')) else 0\n",
        "    volume_weight = ensure_valid_weight((row.get('VOLUME', 0) / np.nanmax([row.get('VOLUME', 1), 1])) * quantitative_weights['VOLUME']) if not pd.isna(row.get('VOLUME')) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        type_weight +\n",
        "        status_weight +\n",
        "        contype_weight +\n",
        "        ie_port_weight +\n",
        "        impexpctry_weight +\n",
        "        storcap_weight +\n",
        "        currentcap_weight +\n",
        "        berths_weight +\n",
        "        storage_weight +\n",
        "        appcap_weight +\n",
        "        volume_weight\n",
        "    )\n",
        "\n",
        "    # Round the total weight to 4 decimal places\n",
        "    return round(ensure_valid_weight(total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation for each LNG terminal...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_lng_terminal_weight, axis=1)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/LNG_Terminals_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3w99DS5zc4O"
      },
      "source": [
        "#### Natural Gas Compressor Stations\n",
        "**Dataset: Natural Gas Compressor Stations -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::natural-gas-compressor-stations/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKsc-vSfzc4O",
        "outputId": "3e102e52-4bd2-4fbd-f1f7-80219df54912"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    'NATURAL GAS COMPRESSOR STATION': 1.000\n",
        "}\n",
        "\n",
        "status_weights = {\n",
        "    'IN SERVICE': 0.800,\n",
        "    'NOT AVAILABLE': 0.100,\n",
        "    'SUSPENDED': 0.070,\n",
        "    'ABANDONED': 0.025,\n",
        "    'CLOSED': 0.005\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.500,\n",
        "    'STATUS': 0.500\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'NUM_UNITS': 0.150,\n",
        "    'CERT_HP': 0.200,\n",
        "    'PLANT_COST': 0.150,\n",
        "    'EXP_FUEL': 0.100,\n",
        "    'EXP_OTHER': 0.100,\n",
        "    'GAS_COMPRE': 0.150,\n",
        "    'OP_NUM_COM': 0.100\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_compressor_station_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = ensure_valid_weight(type_weights.get(row.get('TYPE', 0), 0.0)) * qualitative_importance_grades['TYPE']\n",
        "    status_weight = ensure_valid_weight(status_weights.get(row.get('STATUS', 0), 0.0)) * qualitative_importance_grades['STATUS']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    num_units_weight = ensure_valid_weight((row.get('NUM_UNITS', 0) / np.nanmax([row.get('NUM_UNITS', 1), 1])) * quantitative_weights['NUM_UNITS']) if not pd.isna(row.get('NUM_UNITS')) else 0\n",
        "    cert_hp_weight = ensure_valid_weight((row.get('CERT_HP', 0) / np.nanmax([row.get('CERT_HP', 1), 1])) * quantitative_weights['CERT_HP']) if not pd.isna(row.get('CERT_HP')) else 0\n",
        "    plant_cost_weight = ensure_valid_weight((row.get('PLANT_COST', 0) / np.nanmax([row.get('PLANT_COST', 1), 1])) * quantitative_weights['PLANT_COST']) if not pd.isna(row.get('PLANT_COST')) else 0\n",
        "    exp_fuel_weight = ensure_valid_weight((row.get('EXP_FUEL', 0) / np.nanmax([row.get('EXP_FUEL', 1), 1])) * quantitative_weights['EXP_FUEL']) if not pd.isna(row.get('EXP_FUEL')) else 0\n",
        "    exp_other_weight = ensure_valid_weight((row.get('EXP_OTHER', 0) / np.nanmax([row.get('EXP_OTHER', 1), 1])) * quantitative_weights['EXP_OTHER']) if not pd.isna(row.get('EXP_OTHER')) else 0\n",
        "    gas_compre_weight = ensure_valid_weight((row.get('GAS_COMPRE', 0) / np.nanmax([row.get('GAS_COMPRE', 1), 1])) * quantitative_weights['GAS_COMPRE']) if not pd.isna(row.get('GAS_COMPRE')) else 0\n",
        "    op_num_com_weight = ensure_valid_weight((row.get('OP_NUM_COM', 0) / np.nanmax([row.get('OP_NUM_COM', 1), 1])) * quantitative_weights['OP_NUM_COM']) if not pd.isna(row.get('OP_NUM_COM')) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        type_weight +\n",
        "        status_weight +\n",
        "        num_units_weight +\n",
        "        cert_hp_weight +\n",
        "        plant_cost_weight +\n",
        "        exp_fuel_weight +\n",
        "        exp_other_weight +\n",
        "        gas_compre_weight +\n",
        "        op_num_com_weight\n",
        "    )\n",
        "\n",
        "    # Round the total weight to 4 decimal places\n",
        "    return round(ensure_valid_weight(total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove unnecessary fields\n",
        "def apply_weights_and_strip_fields(gdf):\n",
        "    print(\"Starting weight calculation for each compressor station...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_compressor_station_weight, axis=1)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_strip_fields(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Gas_Compressor_Stations_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU0VchVMzc4O"
      },
      "source": [
        "#### Natural Gas Pipelines\n",
        "**Dataset: Natural Gas Pipelines -** https://hifld-geoplatform.opendata.arcgis.com/datasets/geoplatform::natural-gas-pipelines/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yIypvERzc4O",
        "outputId": "bbe5e12c-ad74-4aa8-9edd-b90909e6b6e3"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "typepipe_weights = {\n",
        "    'Interstate': 0.600,\n",
        "    'Intrastate': 0.350,\n",
        "    'Gathering': 0.040,\n",
        "    'Other': 0.010\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPEPIPE': 1.000\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'Shape_Leng': 1.000\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_pipeline_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    typepipe_weight = ensure_valid_weight(typepipe_weights.get(row.get('TYPEPIPE', 'Other'), 0.0)) * qualitative_importance_grades['TYPEPIPE']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    length_weight = ensure_valid_weight((row.get('Shape_Leng', 0) / np.nanmax([row.get('Shape_Leng', 1), 1])) * quantitative_weights['Shape_Leng']) if not pd.isna(row.get('Shape_Leng')) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = typepipe_weight + length_weight\n",
        "\n",
        "    # Round the total weight to 4 decimal places\n",
        "    return round(ensure_valid_weight(total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights, remove features with null geometry, and strip unnecessary fields\n",
        "def apply_weights_and_filter(gdf):\n",
        "    print(\"Starting weight calculation for each pipeline segment...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_pipeline_weight, axis=1)\n",
        "\n",
        "    # Filter out rows with null geometry\n",
        "    gdf_filtered = gdf[gdf.geometry.notnull()].copy()\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf_filtered[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation, filtering null geometries, and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights, filter null geometries, and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_filter(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/NGL_Pipelines_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4PgfHgjzc4P"
      },
      "source": [
        "#### Natural Gas Processing Plants\n",
        "**Dataset: Natural Gas Processing Plants -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::natural-gas-processing-plants/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qB8Vdejzc4P",
        "outputId": "1b01445a-3e4f-4d68-f132-a6172eb8e955"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    'NATURAL GAS LIQUIDS EXTRACTION PLANT': 0.300,\n",
        "    'NATURAL GAS PROCESSING PLANT': 0.250,\n",
        "    'NOT AVAILABLE': 0.100,\n",
        "    'NATURAL GAS PROCESSING AND NATURAL GAS LIQUIDS EXTRACTION PLANT': 0.150,\n",
        "    'CRUDE PETROLEUM AND NATURAL GAS EXTRACTION PLANT': 0.125,\n",
        "    'SWEET NATURAL GAS PROCESSING PLANT': 0.075\n",
        "}\n",
        "\n",
        "status_weights = {\n",
        "    'ACTIVE': 0.800,\n",
        "    'NOT AVAILABLE': 0.150,\n",
        "    'INACTIVE': 0.050\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.500,\n",
        "    'STATUS': 0.500\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'GASCAP': 0.250,\n",
        "    'PLANTFLOW': 0.250,\n",
        "    'BTUCONTENT': 0.150,\n",
        "    'PROCAMTBLS': 0.150,\n",
        "    'GASSTORCAP': 0.100,\n",
        "    'LIQSTORCAP': 0.100\n",
        "}\n",
        "\n",
        "# Function to ensure valid weight value (no NULL or negative weights)\n",
        "def ensure_valid_weight(weight):\n",
        "    if pd.isnull(weight) or weight < 0:\n",
        "        return 0.0  # Set any invalid weight to 0.0\n",
        "    return float(max(weight, 0.0))  # Ensure the weight is non-negative\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_ng_processing_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = ensure_valid_weight(type_weights.get(row.get('TYPE', 'NOT AVAILABLE'), 0.0)) * qualitative_importance_grades['TYPE']\n",
        "    status_weight = ensure_valid_weight(status_weights.get(row.get('STATUS', 'NOT AVAILABLE'), 0.0)) * qualitative_importance_grades['STATUS']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    gascap_weight = ensure_valid_weight((row.get('GASCAP', 0) / np.nanmax([row.get('GASCAP', 1), 1])) * quantitative_weights['GASCAP']) if not pd.isna(row.get('GASCAP')) else 0\n",
        "    plantflow_weight = ensure_valid_weight((row.get('PLANTFLOW', 0) / np.nanmax([row.get('PLANTFLOW', 1), 1])) * quantitative_weights['PLANTFLOW']) if not pd.isna(row.get('PLANTFLOW')) else 0\n",
        "    btucontent_weight = ensure_valid_weight((row.get('BTUCONTENT', 0) / np.nanmax([row.get('BTUCONTENT', 1), 1])) * quantitative_weights['BTUCONTENT']) if not pd.isna(row.get('BTUCONTENT')) else 0\n",
        "    procamtbls_weight = ensure_valid_weight((row.get('PROCAMTBLS', 0) / np.nanmax([row.get('PROCAMTBLS', 1), 1])) * quantitative_weights['PROCAMTBLS']) if not pd.isna(row.get('PROCAMTBLS')) else 0\n",
        "    gasstorcap_weight = ensure_valid_weight((row.get('GASSTORCAP', 0) / np.nanmax([row.get('GASSTORCAP', 1), 1])) * quantitative_weights['GASSTORCAP']) if not pd.isna(row.get('GASSTORCAP')) else 0\n",
        "    liqstorcap_weight = ensure_valid_weight((row.get('LIQSTORCAP', 0) / np.nanmax([row.get('LIQSTORCAP', 1), 1])) * quantitative_weights['LIQSTORCAP']) if not pd.isna(row.get('LIQSTORCAP')) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        type_weight +\n",
        "        status_weight +\n",
        "        gascap_weight +\n",
        "        plantflow_weight +\n",
        "        btucontent_weight +\n",
        "        procamtbls_weight +\n",
        "        gasstorcap_weight +\n",
        "        liqstorcap_weight\n",
        "    )\n",
        "\n",
        "    # Round the total weight to 4 decimal places\n",
        "    return round(ensure_valid_weight(total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights, remove features with null geometry, and strip unnecessary fields\n",
        "def apply_weights_and_filter(gdf):\n",
        "    print(\"Starting weight calculation for each natural gas processing plant...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_ng_processing_weight, axis=1)\n",
        "\n",
        "    # Filter out rows with null geometry\n",
        "    gdf_filtered = gdf[gdf.geometry.notnull()].copy()\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf_filtered[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation, filtering null geometries, and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights, filter null geometries, and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_filter(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/NG_Processing_Plants_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXj0QC-qzc4P"
      },
      "source": [
        "#### Petroleum Terminals\n",
        "**Dataset: Petroleum Terminals -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::petroleum-terminals/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J7MQJSRzc4P",
        "outputId": "ff3766b3-2edf-42e0-e0ed-f834258e97c1"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    'BULK TERMINAL': 0.300,\n",
        "    'MARINE TERMINAL': 0.250,\n",
        "    'REFINERY TERMINAL': 0.100,\n",
        "    'Other Types': 0.350\n",
        "}\n",
        "\n",
        "status_weights = {\n",
        "    'IN SERVICE': 0.950,\n",
        "    'DISMANTLED': 0.030,\n",
        "    'NOT IN USE': 0.020\n",
        "}\n",
        "\n",
        "commodity_weights = {\n",
        "    'REFINED': 0.250,\n",
        "    'REFINED, CHEMICALS': 0.150,\n",
        "    'NOT AVAILABLE': 0.050,\n",
        "    'CRUDE': 0.100,\n",
        "    'Other Commodities': 0.450\n",
        "}\n",
        "\n",
        "posrel_weights = {\n",
        "    'WITHIN 40 FEET': 0.900,\n",
        "    'WITHIN 1 MILE': 0.050,\n",
        "    'Other Proximities': 0.050\n",
        "}\n",
        "\n",
        "transport_in_weights = {\n",
        "    'YES': 0.700,\n",
        "    'NO': 0.250,\n",
        "    'NOT AVAILABLE': 0.050\n",
        "}\n",
        "\n",
        "transport_out_weights = {\n",
        "    'YES': 0.700,\n",
        "    'NO': 0.250,\n",
        "    'NOT AVAILABLE': 0.050\n",
        "}\n",
        "\n",
        "# Define individual commodity-specific weights\n",
        "commodity_specific_weights = {\n",
        "    'ASPHALT': 0.080,\n",
        "    'CHEMICALS': 0.100,\n",
        "    'PROPANE': 0.050,\n",
        "    'BUTANE': 0.040,\n",
        "    'REFINED': 0.200,\n",
        "    'ETHANOL': 0.060,\n",
        "    'BIODIESEL': 0.050,\n",
        "    'CRUDE_OIL': 0.150,\n",
        "    'JETFUEL': 0.100,\n",
        "    'GASOLINE': 0.120,\n",
        "    'DISTILLATE': 0.200,\n",
        "    'AVGAS': 0.050\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.200,\n",
        "    'STATUS': 0.200,\n",
        "    'COMMODITY': 0.150,\n",
        "    'POSREL': 0.100,\n",
        "    'TRUCK_IN': 0.050,\n",
        "    'TRUCK_OUT': 0.050,\n",
        "    'PIPE_IN': 0.050,\n",
        "    'PIPE_OUT': 0.050,\n",
        "    'MARINE_IN': 0.050,\n",
        "    'MARINE_OUT': 0.050,\n",
        "    'RAIL_IN': 0.050,\n",
        "    'RAIL_OUT': 0.050,\n",
        "    'COMMODITIES': 0.100\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'CAPACITY': 0.200\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_terminal_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = type_weights.get(row['TYPE'], type_weights['Other Types']) * qualitative_importance_grades['TYPE']\n",
        "    status_weight = status_weights.get(row['STATUS'], 0.0) * qualitative_importance_grades['STATUS']\n",
        "    commodity_weight = commodity_weights.get(row['COMMODITY'], commodity_weights['Other Commodities']) * qualitative_importance_grades['COMMODITY']\n",
        "    posrel_weight = posrel_weights.get(row['POSREL'], posrel_weights['Other Proximities']) * qualitative_importance_grades['POSREL']\n",
        "\n",
        "    # Transport modes\n",
        "    truck_in_weight = transport_in_weights.get(row['TRUCK_IN'], 0.0) * qualitative_importance_grades['TRUCK_IN']\n",
        "    truck_out_weight = transport_out_weights.get(row['TRUCK_OUT'], 0.0) * qualitative_importance_grades['TRUCK_OUT']\n",
        "    pipe_in_weight = transport_in_weights.get(row['PIPE_IN'], 0.0) * qualitative_importance_grades['PIPE_IN']\n",
        "    pipe_out_weight = transport_out_weights.get(row['PIPE_OUT'], 0.0) * qualitative_importance_grades['PIPE_OUT']\n",
        "    marine_in_weight = transport_in_weights.get(row['MARINE_IN'], 0.0) * qualitative_importance_grades['MARINE_IN']\n",
        "    marine_out_weight = transport_out_weights.get(row['MARINE_OUT'], 0.0) * qualitative_importance_grades['MARINE_OUT']\n",
        "    rail_in_weight = transport_in_weights.get(row['RAIL_IN'], 0.0) * qualitative_importance_grades['RAIL_IN']\n",
        "    rail_out_weight = transport_out_weights.get(row['RAIL_OUT'], 0.0) * qualitative_importance_grades['RAIL_OUT']\n",
        "\n",
        "    # Commodity-specific weights\n",
        "    commodities_weight = sum([commodity_specific_weights.get(commodity, 0.0) * qualitative_importance_grades['COMMODITIES'] for commodity in commodity_specific_weights if row.get(commodity, 'NO') == 'YES'])\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    capacity_weight = (row['CAPACITY'] / np.nanmax(row['CAPACITY'])) * quantitative_weights['CAPACITY'] if not pd.isna(row['CAPACITY']) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        type_weight +\n",
        "        status_weight +\n",
        "        commodity_weight +\n",
        "        posrel_weight +\n",
        "        truck_in_weight +\n",
        "        truck_out_weight +\n",
        "        pipe_in_weight +\n",
        "        pipe_out_weight +\n",
        "        marine_in_weight +\n",
        "        marine_out_weight +\n",
        "        rail_in_weight +\n",
        "        rail_out_weight +\n",
        "        commodities_weight +\n",
        "        capacity_weight\n",
        "    )\n",
        "\n",
        "    # Ensure valid and positive weight\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights, remove features with null geometry, and strip unnecessary fields\n",
        "def apply_weights_and_filter(gdf):\n",
        "    print(\"Starting weight calculation for each petroleum terminal...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_terminal_weight, axis=1)\n",
        "\n",
        "    # Filter out rows with null geometry\n",
        "    gdf_filtered = gdf[gdf.geometry.notnull()].copy()\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf_filtered[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation, filtering null geometries, and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights, filter null geometries, and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_filter(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Petroleum_Terminals_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKq6eSvzzc4Q"
      },
      "source": [
        "### Oil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B6jFGYczc4Q"
      },
      "source": [
        "#### Natural Gas Wells\n",
        "**Dataset: Oil and Natural Gas Wells -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::oil-and-natural-gas-wells/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXYMeXhnzc4Q"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def calculate_well_importance(well):\n",
        "    importance_score = 0.0\n",
        "\n",
        "    # Weights for each attribute\n",
        "    weights = {\n",
        "        'status_weight': 0.25,\n",
        "        'prodtype_weight': 0.25,\n",
        "        'compdate_weight': 0.25,\n",
        "        'totdepth_weight': 0.25,\n",
        "    }\n",
        "\n",
        "    # Status importance mapping\n",
        "    status_importance = {\n",
        "        'PRODUCING WELL': 1.0,\n",
        "        'ACTIVE WELL': 0.8,\n",
        "        'NON-ACTIVE WELL': 0.4,\n",
        "        'UNKNOWN WELL': 0.2,\n",
        "        'WELL DEVELOPMENT': 0.6,\n",
        "        'STORAGE WELL/MAINTENANCE WELL/OBSERVATION WELL': 0.5,\n",
        "        'PRODUCING, NON-ACTIVE WELL': 0.7,\n",
        "    }\n",
        "\n",
        "    # Production type importance mapping\n",
        "    prodtype_importance = {\n",
        "        'OIL': 1.0,\n",
        "        'GAS': 0.9,\n",
        "        'OIL & NATURAL GAS': 1.0,\n",
        "        'UNKNOWN': 0.2,\n",
        "    }\n",
        "\n",
        "    # Normalize and weigh the well's status\n",
        "    status_score = status_importance.get(well['STATUS'], 0)\n",
        "    importance_score += status_score * weights['status_weight']\n",
        "\n",
        "    # Normalize and weigh the production type\n",
        "    prodtype_score = prodtype_importance.get(well['PRODTYPE'], 0)\n",
        "    importance_score += prodtype_score * weights['prodtype_weight']\n",
        "\n",
        "    # Completion date factor (more recent = higher score)\n",
        "    try:\n",
        "        compdate = datetime.strptime(well['COMPDATE'], '%m/%d/%Y %I:%M:%S %p')\n",
        "        years_since_completion = (datetime.now() - compdate).days / 365.25\n",
        "        compdate_score = max(1 - (years_since_completion / 100), 0)  # Assuming a 100-year relevance span\n",
        "    except ValueError:  # In case of a date parsing error or -999 values\n",
        "        compdate_score = 0\n",
        "    importance_score += compdate_score * weights['compdate_weight']\n",
        "\n",
        "    # Total depth factor (deeper wells might be more significant due to larger investments)\n",
        "    try:\n",
        "        totdepth = float(well['TOTDEPTH'])\n",
        "        max_depth = 30000  # Example max depth for normalization\n",
        "        totdepth_score = min(totdepth / max_depth, 1)\n",
        "    except ValueError:  # In case of a parsing error or -999 values\n",
        "        totdepth_score = 0\n",
        "    importance_score += totdepth_score * weights['totdepth_weight']\n",
        "\n",
        "    # Ensure the score does not exceed 1.0\n",
        "    importance_score = min(importance_score, 1.0)\n",
        "\n",
        "    return round(importance_score, 3)\n",
        "\n",
        "# Example usage with a hypothetical well data\n",
        "example_well = {\n",
        "    'STATUS': 'PRODUCING WELL',\n",
        "    'PRODTYPE': 'OIL & NATURAL GAS',\n",
        "    'COMPDATE': '4/9/1951 12:00:00 AM',\n",
        "    'TOTDEPTH': 15000,\n",
        "}\n",
        "\n",
        "importance_score = calculate_well_importance(example_well)\n",
        "print(\"Importance Score:\", importance_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiLxfFwcZ9NC"
      },
      "source": [
        "#### Oil Wells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HatkqFt0Z9VQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5cCBY92zc4R"
      },
      "source": [
        "#### Oil Refineries\n",
        "**Dataset: Oil Refineries -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::oil-refineries/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdTRm1nGzc4R",
        "outputId": "61844f8e-45d1-42aa-be62-5170ad52c009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from /content/drive/MyDrive/CI_Sentinal/geoJSON/cleaned/Oil_Refineries_Cleaned.geojson...\n",
            "Dataset loaded successfully.\n",
            "Initial file size: 0.10 MB\n",
            "Starting to calculate weights for each refinery...\n",
            "Weights calculated. Time elapsed: 0.42 seconds.\n",
            "Removing unnecessary fields, keeping only Weight and geometry...\n",
            "Saving weighted dataset to /content/drive/MyDrive/CI_Sentinal/geoJSON/cleaned_weighted/Oil_Refineries_Cleaned_weighted_cleaned.geojson...\n",
            "Final file size: 0.02 MB\n",
            "File size reduced by: 0.08 MB (78.50% reduction).\n"
          ]
        }
      ],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    'LARGE COMPLEX REFINERY': 0.250,\n",
        "    'MEDIUM SIZED MEDIUM COMPLEXITY REFINERY': 0.150,\n",
        "    'VERY LARGE COMPLEX REFINERY': 0.200,\n",
        "    'Other': 0.400\n",
        "}\n",
        "\n",
        "status_weights = {\n",
        "    'IN SERVICE': 0.900,\n",
        "    'CLOSED': 0.100\n",
        "}\n",
        "\n",
        "posrel_weights = {\n",
        "    'WITHIN 166 FEET': 0.900,\n",
        "    'WITHIN 1 MILE': 0.080,\n",
        "    'WITHIN 40 FEET': 0.020\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.300,\n",
        "    'STATUS': 0.400,\n",
        "    'POSREL': 0.300\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'CAPACITY': 0.200,\n",
        "    'US_RANK': 0.100,\n",
        "    'CRUDE': 0.150,\n",
        "    'VACDIST': 0.100,\n",
        "    'COKING': 0.050,\n",
        "    'THERMALOP': 0.020,\n",
        "    'CATCRACK': 0.100,\n",
        "    'CATREFORM': 0.050,\n",
        "    'CATHYDCRCK': 0.050,\n",
        "    'CATHYDTRT': 0.100,\n",
        "    'ALKY': 0.030,\n",
        "    'POLDIM': 0.010,\n",
        "    'AROMATIC': 0.020,\n",
        "    'ISOMER': 0.030,\n",
        "    'LUBES': 0.020,\n",
        "    'OXYGENATES': 0.020,\n",
        "    'HYDRGN': 0.010,\n",
        "    'COKE': 0.030,\n",
        "    'SULFUR': 0.020,\n",
        "    'ASPHALT': 0.020\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_refinery_weight(row, gdf):\n",
        "    # Use 0 if any field is missing (i.e., NaN or not present)\n",
        "    type_weight = type_weights.get(row.get('TYPE', 'Other'), 0.0) * qualitative_importance_grades['TYPE']\n",
        "    status_weight = status_weights.get(row.get('STATUS', ''), 0.0) * qualitative_importance_grades['STATUS']\n",
        "    posrel_weight = posrel_weights.get(row.get('POSREL', ''), 0.0) * qualitative_importance_grades['POSREL']\n",
        "\n",
        "    # Calculate quantitative weights with default value 0 for missing fields\n",
        "    cap_weight = (row.get('CAPACITY', 0) / np.nanmax(gdf['CAPACITY'])) * quantitative_weights['CAPACITY'] if pd.notna(row.get('CAPACITY')) else 0\n",
        "    rank_weight = (row.get('US_RANK', 0) / np.nanmax(gdf['US_RANK'])) * quantitative_weights['US_RANK'] if pd.notna(row.get('US_RANK')) else 0\n",
        "    crude_weight = (row.get('CRUDE', 0) / np.nanmax(gdf['CRUDE'])) * quantitative_weights['CRUDE'] if pd.notna(row.get('CRUDE')) else 0\n",
        "    vacdist_weight = (row.get('VACDIST', 0) / np.nanmax(gdf['VACDIST'])) * quantitative_weights['VACDIST'] if pd.notna(row.get('VACDIST')) else 0\n",
        "    coking_weight = (row.get('COKING', 0) / np.nanmax(gdf['COKING'])) * quantitative_weights['COKING'] if pd.notna(row.get('COKING')) else 0\n",
        "    thermalop_weight = (row.get('THERMALOP', 0) / np.nanmax(gdf['THERMALOP'])) * quantitative_weights['THERMALOP'] if pd.notna(row.get('THERMALOP')) else 0\n",
        "    catcrack_weight = (row.get('CATCRACK', 0) / np.nanmax(gdf['CATCRACK'])) * quantitative_weights['CATCRACK'] if pd.notna(row.get('CATCRACK')) else 0\n",
        "    catreform_weight = (row.get('CATREFORM', 0) / np.nanmax(gdf['CATREFORM'])) * quantitative_weights['CATREFORM'] if pd.notna(row.get('CATREFORM')) else 0\n",
        "    cathydcrck_weight = (row.get('CATHYDCRCK', 0) / np.nanmax(gdf['CATHYDCRCK'])) * quantitative_weights['CATHYDCRCK'] if pd.notna(row.get('CATHYDCRCK')) else 0\n",
        "    cathydtrt_weight = (row.get('CATHYDTRT', 0) / np.nanmax(gdf['CATHYDTRT'])) * quantitative_weights['CATHYDTRT'] if pd.notna(row.get('CATHYDTRT')) else 0\n",
        "    alky_weight = (row.get('ALKY', 0) / np.nanmax(gdf['ALKY'])) * quantitative_weights['ALKY'] if pd.notna(row.get('ALKY')) else 0\n",
        "    poldim_weight = (row.get('POLDIM', 0) / np.nanmax(gdf['POLDIM'])) * quantitative_weights['POLDIM'] if pd.notna(row.get('POLDIM')) else 0\n",
        "    aromatic_weight = (row.get('AROMATIC', 0) / np.nanmax(gdf['AROMATIC'])) * quantitative_weights['AROMATIC'] if pd.notna(row.get('AROMATIC')) else 0\n",
        "    isomer_weight = (row.get('ISOMER', 0) / np.nanmax(gdf['ISOMER'])) * quantitative_weights['ISOMER'] if pd.notna(row.get('ISOMER')) else 0\n",
        "    lubes_weight = (row.get('LUBES', 0) / np.nanmax(gdf['LUBES'])) * quantitative_weights['LUBES'] if pd.notna(row.get('LUBES')) else 0\n",
        "    oxygenates_weight = (row.get('OXYGENATES', 0) / np.nanmax(gdf['OXYGENATES'])) * quantitative_weights['OXYGENATES'] if pd.notna(row.get('OXYGENATES')) else 0\n",
        "    hydrgn_weight = (row.get('HYDRGN', 0) / np.nanmax(gdf['HYDRGN'])) * quantitative_weights['HYDRGN'] if pd.notna(row.get('HYDRGN')) else 0\n",
        "    coke_weight = (row.get('COKE', 0) / np.nanmax(gdf['COKE'])) * quantitative_weights['COKE'] if pd.notna(row.get('COKE')) else 0\n",
        "    sulfur_weight = (row.get('SULFUR', 0) / np.nanmax(gdf['SULFUR'])) * quantitative_weights['SULFUR'] if pd.notna(row.get('SULFUR')) else 0\n",
        "    asphalt_weight = (row.get('ASPHALT', 0) / np.nanmax(gdf['ASPHALT'])) * quantitative_weights['ASPHALT'] if pd.notna(row.get('ASPHALT')) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        type_weight +\n",
        "        status_weight +\n",
        "        posrel_weight +\n",
        "        cap_weight +\n",
        "        rank_weight +\n",
        "        crude_weight +\n",
        "        vacdist_weight +\n",
        "        coking_weight +\n",
        "        thermalop_weight +\n",
        "        catcrack_weight +\n",
        "        catreform_weight +\n",
        "        cathydcrck_weight +\n",
        "        cathydtrt_weight +\n",
        "        alky_weight +\n",
        "        poldim_weight +\n",
        "        aromatic_weight +\n",
        "        isomer_weight +\n",
        "        lubes_weight +\n",
        "        oxygenates_weight +\n",
        "        hydrgn_weight +\n",
        "        coke_weight +\n",
        "        sulfur_weight +\n",
        "        asphalt_weight\n",
        "    )\n",
        "\n",
        "    return round(total_weight, 4)  # Ensure 4 decimal places and always return a number\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_refineries_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each refinery...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row, passing gdf for context\n",
        "    gdf['Weight'] = gdf.apply(lambda row: calculate_refinery_weight(row, gdf), axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Remove unwanted fields and keep only Weight and geometry\n",
        "def remove_unnecessary_fields(gdf):\n",
        "    print(\"Removing unnecessary fields, keeping only Weight and geometry...\")\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "    return gdf\n",
        "\n",
        "# Step 5: Save the cleaned and weighted dataset\n",
        "def process_and_save_weighted_geojson(input_path, output_path):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Size in MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_refineries_dataset(gdf)\n",
        "\n",
        "    # Remove unnecessary fields (keep only 'Weight' and 'geometry')\n",
        "    gdf_cleaned = remove_unnecessary_fields(gdf_weighted)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_cleaned.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Size in MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Step 6: Define paths and process the dataset\n",
        "input_path = \"/geoJSON/cleaned/Oil_Refineries_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Construct the output path\n",
        "output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g2TJSmSzc4R"
      },
      "source": [
        "## Financials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz9LXPP0zc4R"
      },
      "source": [
        "### County Business Patterns\n",
        "**Dataset: County Business Patterns -** https://hub.arcgis.com/datasets/USCensus::county-business-patterns-counties-2021/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT9XOqM7zc4R",
        "outputId": "951130a0-95dd-40d2-edd8-831af8f24776"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "statefp_weights = {\n",
        "    '48': 0.150,  # Texas\n",
        "    '13': 0.100,  # Georgia\n",
        "    'Other': 0.750  # Other states collectively\n",
        "}\n",
        "\n",
        "lsad_weights = {\n",
        "    '06': 0.800,\n",
        "    '13': 0.100,\n",
        "    'Other': 0.100\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'STATEFP': 0.500,\n",
        "    'LSAD': 0.500\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'ESTAB_00': 0.200,\n",
        "    'ESTAB_21': 0.020,\n",
        "    'ESTAB_22': 0.020,\n",
        "    'ESTAB_23': 0.050,\n",
        "    'ESTAB_31_33': 0.050,\n",
        "    'ESTAB_42': 0.050,\n",
        "    'ESTAB_44_45': 0.070,\n",
        "    'ESTAB_48_49': 0.040,\n",
        "    'ESTAB_51': 0.040,\n",
        "    'ESTAB_52': 0.050,\n",
        "    'ESTAB_53': 0.050,\n",
        "    'ESTAB_54': 0.070,\n",
        "    'ESTAB_55': 0.030,\n",
        "    'ESTAB_56': 0.050,\n",
        "    'ESTAB_61': 0.040,\n",
        "    'ESTAB_62': 0.070,\n",
        "    'ESTAB_71': 0.040,\n",
        "    'ESTAB_72': 0.050,\n",
        "    'ESTAB_81': 0.050,\n",
        "    'PAYANN_00': 0.070,\n",
        "    'PAYANN_21': 0.020,\n",
        "    'PAYANN_22': 0.020,\n",
        "    'PAYANN_23': 0.040,\n",
        "    'PAYANN_31_33': 0.070,\n",
        "    'PAYANN_42': 0.050,\n",
        "    'PAYANN_44_45': 0.050,\n",
        "    'PAYANN_48_49': 0.050,\n",
        "    'PAYANN_51': 0.060,\n",
        "    'PAYANN_52': 0.060,\n",
        "    'PAYANN_53': 0.040,\n",
        "    'PAYANN_54': 0.060,\n",
        "    'PAYANN_55': 0.060,\n",
        "    'PAYANN_56': 0.050,\n",
        "    'PAYANN_61': 0.050,\n",
        "    'PAYANN_62': 0.060,\n",
        "    'PAYANN_71': 0.040,\n",
        "    'PAYANN_72': 0.050,\n",
        "    'PAYANN_81': 0.040,\n",
        "    'PAYANNAVG_CALC': 0.050\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_cbp_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    statefp_weight = statefp_weights.get(row['STATEFP'], statefp_weights['Other']) * qualitative_importance_grades['STATEFP']\n",
        "    lsad_weight = lsad_weights.get(row['LSAD'], lsad_weights['Other']) * qualitative_importance_grades['LSAD']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    quantitative_weight = 0\n",
        "    for column, weight in quantitative_weights.items():\n",
        "        # Ensure the column value is numeric and skip non-numeric data\n",
        "        if pd.api.types.is_numeric_dtype(row[column]):\n",
        "            value = row.get(column, 0)  # Default to 0 if missing\n",
        "            max_value = np.nanmax([value if value is not None else 0 for value in row[column:]])\n",
        "            if max_value > 0:\n",
        "                quantitative_weight += (value / max_value) * weight if value is not None else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = statefp_weight + lsad_weight + quantitative_weight\n",
        "\n",
        "    # Ensure weight is positive and rounded to 4 decimal places\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove any null geometries\n",
        "\n",
        "def apply_weights_and_filter(gdf):\n",
        "    print(\"Starting to calculate weights for each County Business Pattern record...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_cbp_weight, axis=1)\n",
        "\n",
        "    # Remove null geometries\n",
        "    gdf_cleaned = gdf[gdf.geometry.notnull()].copy()\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_cleaned\n",
        "\n",
        "# Step 4: Process and save the dataset with weights and remove unnecessary fields\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and remove null geometries\n",
        "    gdf_weighted = apply_weights_and_filter(gdf)\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' columns\n",
        "    gdf_weighted = gdf_weighted[['Weight', 'geometry']]\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/CBP_2021_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbqebaHmzc4S"
      },
      "source": [
        "### FDIC Insured Banks\n",
        "**Dataset: FDIC Insured Banks -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::fdic-insured-banks/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY19adYnzc4S",
        "outputId": "7e038fab-eaca-499e-cb84-64e5825b7f19"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define BKCLASS weights\n",
        "bkclass_weights = {\n",
        "    'N': 0.3,  # National bank class\n",
        "    'SM': 0.2,  # State Member\n",
        "    'NM': 0.1,  # Non-Member\n",
        "    'SI': 0.1,  # Savings Institution\n",
        "    'Other': 0.05\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'BKCLASS': 0.1\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'ASSET': 0.5,\n",
        "    'DEPDOM': 0.4\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_bank_weight(row):\n",
        "    # Ensure valid fields with default to 0 for missing values\n",
        "    asset_value = row.get('ASSET', 0.0)\n",
        "    depdom_value = row.get('DEPDOM', 0.0)\n",
        "    bkclass_value = row.get('BKCLASS', 'Other')  # Default to 'Other' if BKCLASS is missing\n",
        "\n",
        "    # Calculate qualitative weights for BKCLASS\n",
        "    bkclass_weight = bkclass_weights.get(bkclass_value, bkclass_weights['Other']) * qualitative_importance_grades['BKCLASS']\n",
        "\n",
        "    # Calculate quantitative weights for ASSET and DEPDOM\n",
        "    max_asset = 1e12  # Example max asset in dollars\n",
        "    max_depdom = 1e11  # Example max domestic deposits in dollars\n",
        "\n",
        "    asset_weight = (asset_value / max_asset) * quantitative_weights['ASSET'] if asset_value > 0 else 0\n",
        "    depdom_weight = (depdom_value / max_depdom) * quantitative_weights['DEPDOM'] if depdom_value > 0 else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = bkclass_weight + asset_weight + depdom_weight\n",
        "\n",
        "    # Ensure valid and positive weight, rounded to 4 decimals\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights, remove features with null geometry, and strip unnecessary fields\n",
        "def apply_weights_and_filter(gdf):\n",
        "    print(\"Starting weight calculation for each FDIC bank...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_bank_weight, axis=1)\n",
        "\n",
        "    # Filter out rows with null geometry\n",
        "    gdf_filtered = gdf[gdf.geometry.notnull()].copy()\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf_filtered[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation, filtering null geometries, and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights, filter null geometries, and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_filter(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/FDIC_Banks_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7St1aRkzc4S"
      },
      "source": [
        "### Gold Bullion Depositories\n",
        "**Dataset: Gold Bullion Depositories -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::gold-bullion-repositories/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkLlaVNkzc4S",
        "outputId": "da654d0d-b1cb-4d27-e8c8-e27c66044778"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "hsipthemes_weights = {\n",
        "    \"CRITICAL INFRASTRUCTURE, PDD-63; BANKING, FINANCE, AND INSURANCE; BULLION REPOSITORIES\": 1.000\n",
        "}\n",
        "\n",
        "naicscode_weights = {\n",
        "    423940: 0.870,\n",
        "    339911: 0.057,\n",
        "    921130: 0.057,\n",
        "    521110: 0.016\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'HSIPTHEMES': 0.500,\n",
        "    'NAICSCODE': 0.500\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_gold_depository_weight(row):\n",
        "    # Ensure valid fields with default to 0 for missing values\n",
        "    hsipthemes_weight = hsipthemes_weights.get(row.get('HSIPTHEMES', ''), 0.0) * qualitative_importance_grades['HSIPTHEMES']\n",
        "    naicscode_weight = naicscode_weights.get(row.get('NAICSCODE', 0), 0.0) * qualitative_importance_grades['NAICSCODE']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = hsipthemes_weight + naicscode_weight\n",
        "\n",
        "    # Ensure valid and positive weight, rounded to 4 decimals\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights and remove features with null geometry\n",
        "\n",
        "def apply_weights_and_filter(gdf):\n",
        "    print(\"Starting to calculate weights for each gold depository...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_gold_depository_weight, axis=1)\n",
        "\n",
        "    # Filter out rows with null geometry\n",
        "    gdf_filtered = gdf[gdf.geometry.notnull()].copy()\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf_filtered[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated, null geometries filtered, and fields stripped in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and filter null geometries\n",
        "    gdf_weighted = apply_weights_and_filter(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Gold_Repositories_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj8TA54Xzc4T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDMTdLuzzc4T"
      },
      "source": [
        "## Food"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdkDwS9gzc4T"
      },
      "source": [
        "### Public Refridgerated Warehouse\n",
        "**Dataset: Public Refrigerated Warehouses -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::public-refrigerated-warehouses/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtS5CTV_zc4T",
        "outputId": "ed5d0d72-f493-46fc-b3ac-fb5cb7ec0580"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "loc_name_weights = {\n",
        "    'US_RoofTop': 0.500,\n",
        "    'US_Streets': 0.400,\n",
        "    'US_Zipcode': 0.050,\n",
        "    'US_StreetName': 0.030,\n",
        "    'US_Zip4': 0.020\n",
        "}\n",
        "\n",
        "# Define overall importance grades for the qualitative properties\n",
        "qualitative_importance_grades = {\n",
        "    'Loc_name': 0.500\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'Volume': 1.000\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_warehouse_weight(row):\n",
        "    # Ensure valid fields with default to 0 for missing values\n",
        "    volume_value = row.get('Volume', 0.0)\n",
        "\n",
        "    # Calculate qualitative weights\n",
        "    loc_name_weight = loc_name_weights.get(row['Loc_name'], 0.0) * qualitative_importance_grades['Loc_name']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    volume_weight = (volume_value / np.nanmax(volume_value)) * quantitative_weights['Volume'] if volume_value > 0 else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = loc_name_weight + volume_weight\n",
        "\n",
        "    # Ensure valid and positive weight, rounded to 4 decimals\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights, remove features with null geometry, and strip unnecessary fields\n",
        "def apply_weights_and_filter(gdf):\n",
        "    print(\"Starting weight calculation for each refrigerated warehouse...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_warehouse_weight, axis=1)\n",
        "\n",
        "    # Filter out rows with null geometry\n",
        "    gdf_filtered = gdf[gdf.geometry.notnull()].copy()\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf_filtered[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation, filtering null geometries, and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights, filter null geometries, and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_filter(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Refrigerated_Warehouses_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znUWgvVUzc4U"
      },
      "source": [
        "## Government"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRj-nY0Tzc4U"
      },
      "source": [
        "### Courthouses\n",
        "**Dataset: Courthouses -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::courthouses-3/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UDU9C-1zc4U",
        "outputId": "f57cf7d1-f5e1-496d-cec3-aa70cccf8113"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Step 1: Define weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "ftype_weights = {\n",
        "    830: 1.0  # Assigning a weight for Courthouses (830)\n",
        "}\n",
        "\n",
        "fcode_weights = {\n",
        "    83011: 1.0  # Assigning a weight for Courthouse FCODE (83011)\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'FTYPE': 0.5,\n",
        "    'FCODE': 0.5\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each courthouse\n",
        "\n",
        "def calculate_courthouse_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    ftype_weight = ftype_weights.get(row.get('FTYPE', 0), 0.0) * qualitative_importance_grades['FTYPE']\n",
        "    fcode_weight = fcode_weights.get(row.get('FCODE', 0), 0.0) * qualitative_importance_grades['FCODE']\n",
        "\n",
        "    # Calculate the total weight as a combination of all factors\n",
        "    total_weight = ftype_weight + fcode_weight\n",
        "\n",
        "    # Ensure valid and positive weight, rounded to 4 decimal places\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights, remove features with null geometry, and strip unnecessary fields\n",
        "\n",
        "def apply_weights_and_filter(gdf):\n",
        "    print(\"Starting weight calculation for each courthouse...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_courthouse_weight, axis=1)\n",
        "\n",
        "    # Filter out rows with null geometry\n",
        "    gdf_filtered = gdf[gdf.geometry.notnull()].copy()\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf_filtered[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weight calculation, filtering null geometries, and field stripping completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights, filter null geometries, and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_filter(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure the final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Courthouses_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wKtF1rHzc4V"
      },
      "source": [
        "### Major State Government Buildings\n",
        "**Dataset: Major State Government Buildings -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::major-state-government-buildings/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abe5DPiRzc4W",
        "outputId": "a222f334-ee0f-418a-a524-16c995540d13"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "naicscode_weights = {\n",
        "    '921190': 1.000  # Government building NAICSCODE\n",
        "}\n",
        "\n",
        "fips_weights = {\n",
        "    '48453': 0.400,  # Example FIPS codes with higher significance\n",
        "    '41047': 0.350,\n",
        "    '51760': 0.250,\n",
        "    'Other': 0.050  # Default weight for other FIPS\n",
        "}\n",
        "\n",
        "agencies_weights = {\n",
        "    'DEPARTMENT OF INFORMATION RESOURCES': 0.150,\n",
        "    'DEPARTMENT OF TRANSPORTATION': 0.140,\n",
        "    'DEPARTMENT OF EDUCATION': 0.135,\n",
        "    'DEPARTMENT OF CORRECTIONS': 0.130,\n",
        "    'SUPREME COURT': 0.125,\n",
        "    'DEPARTMENT OF REVENUE': 0.120,\n",
        "    'ATTORNEY GENERAL': 0.100,\n",
        "    'Other': 0.100\n",
        "}\n",
        "\n",
        "num_agency_weights = {\n",
        "    1: 0.500,\n",
        "    2: 0.200,\n",
        "    3: 0.150,\n",
        "    4: 0.080,\n",
        "    5: 0.050,\n",
        "    'NOT AVAILABLE': 0.020\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'NAICSCODE': 0.350,\n",
        "    'FIPS': 0.250,\n",
        "    'AGENCIES': 0.250,\n",
        "    'NUM_AGENCY': 0.150\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_state_gov_building_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    naicscode_weight = naicscode_weights.get(row.get('NAICSCODE', 'Other'), 0.0) * qualitative_importance_grades['NAICSCODE']\n",
        "    fips_weight = fips_weights.get(row.get('FIPS', 'Other'), fips_weights['Other']) * qualitative_importance_grades['FIPS']\n",
        "    agencies_weight = agencies_weights.get(row.get('AGENCIES', 'Other'), agencies_weights['Other']) * qualitative_importance_grades['AGENCIES']\n",
        "\n",
        "    # Handle 'NUM_AGENCY', ensuring it's a valid integer or treat as 'NOT AVAILABLE'\n",
        "    num_agency_value = row.get('NUM_AGENCY', 'NOT AVAILABLE')\n",
        "    if num_agency_value.isdigit():\n",
        "        num_agency_value = int(num_agency_value)\n",
        "    else:\n",
        "        num_agency_value = 'NOT AVAILABLE'\n",
        "\n",
        "    num_agency_weight = num_agency_weights.get(num_agency_value, num_agency_weights['NOT AVAILABLE']) * qualitative_importance_grades['NUM_AGENCY']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = (\n",
        "        naicscode_weight +\n",
        "        fips_weight +\n",
        "        agencies_weight +\n",
        "        num_agency_weight\n",
        "    )\n",
        "\n",
        "    # Ensure valid and positive weight, rounded to 4 decimals\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights, remove features with null geometry, and keep only required fields\n",
        "def apply_weights_and_filter(gdf):\n",
        "    print(\"Starting to calculate weights for each state government building...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_state_gov_building_weight, axis=1)\n",
        "\n",
        "    # Filter out rows with null geometry\n",
        "    gdf_filtered = gdf[gdf.geometry.notnull()].copy()\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf_filtered[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated, null geometries filtered, and unnecessary fields removed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights, filter null geometries, and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_filter(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/State_Government_Buildings_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D7pOKC0zc4W"
      },
      "source": [
        "### State Capital Buildings\n",
        "**Dataset: State Capital Buildings -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::state-capitol-buildings-1/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbCBQfPfzc4W",
        "outputId": "fc5cca56-3dca-4977-8288-ab0f868f0ae6"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "distribution_policy_weights = {\n",
        "    'E4': 0.500,\n",
        "    'E3': 0.300,\n",
        "    'E2': 0.150,\n",
        "    'Other': 0.050\n",
        "}\n",
        "\n",
        "data_security_weights = {\n",
        "    5: 0.400,\n",
        "    4: 0.300,\n",
        "    3: 0.200,\n",
        "    2: 0.100,\n",
        "    1: 0.050\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'DISTRIBUTION_POLICY': 0.500,\n",
        "    'DATA_SECURITY': 0.500\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_state_capitol_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    distribution_policy_weight = distribution_policy_weights.get(row['DISTRIBUTION_POLICY'], distribution_policy_weights['Other']) * qualitative_importance_grades['DISTRIBUTION_POLICY']\n",
        "    data_security_weight = data_security_weights.get(row['DATA_SECURITY'], 0.0) * qualitative_importance_grades['DATA_SECURITY']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = distribution_policy_weight + data_security_weight\n",
        "\n",
        "    # Ensure valid and positive weight, rounded to 4 decimals\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights, remove features with null geometry, and keep only required fields\n",
        "def apply_weights_and_filter(gdf):\n",
        "    print(\"Starting to calculate weights for each state capitol building...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_state_capitol_weight, axis=1)\n",
        "\n",
        "    # Filter out rows with null geometry\n",
        "    gdf_filtered = gdf[gdf.geometry.notnull()].copy()\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf_filtered[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated, null geometries filtered, and unnecessary fields removed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights, filter null geometries, and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_filter(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/State_Capitol_Buildings_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4evf5k0zc4X"
      },
      "source": [
        "### US Army Corp of Engineers Offices\n",
        "**Dataset: US Army Corps of Engineers (USACE) Offices -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::us-army-corps-of-engineers-usace-offices-/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVnZ3DLozc4X",
        "outputId": "dd0861c2-59f0-4d66-a35f-a5156bda88f0"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights for HQ_DIVISIO\n",
        "hq_divisio_weights = {\n",
        "    'Labs & Centers': 0.150,\n",
        "    'CELRD': 0.150,\n",
        "    'CENAD': 0.130,\n",
        "    'CEMVD': 0.130,\n",
        "    'CESAD': 0.110,\n",
        "    'CENWD': 0.110,\n",
        "    'CEPOD': 0.080,\n",
        "    'CESPD': 0.080,\n",
        "    'CESWD': 0.080,\n",
        "    'HQUSACE': 0.040,\n",
        "    'CETAD': 0.030,\n",
        "    '249TH': 0.010\n",
        "}\n",
        "\n",
        "# Define qualitative attribute weights for TIME_ZONE\n",
        "time_zone_weights = {\n",
        "    'Eastern': 0.400,\n",
        "    'Central': 0.350,\n",
        "    'Pacific': 0.120,\n",
        "    'Hawaii': 0.030,\n",
        "    'Afghanistan': 0.010,\n",
        "    'Alaska': 0.010,\n",
        "    'Mountain': 0.010,\n",
        "    'Korea': 0.010,\n",
        "    'Japan': 0.010,\n",
        "    'Other': 0.010\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'HQ_DIVISIO': 0.650,\n",
        "    'TIME_ZONE': 0.350\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_usace_office_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    hq_divisio_weight = hq_divisio_weights.get(row.get('HQ_DIVISIO', 'Other'), 0.0) * qualitative_importance_grades['HQ_DIVISIO']\n",
        "    time_zone_weight = time_zone_weights.get(row.get('TIME_ZONE', 'Other'), 0.0) * qualitative_importance_grades['TIME_ZONE']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = hq_divisio_weight + time_zone_weight\n",
        "\n",
        "    # Ensure the total weight is positive and rounded to 4 decimal places\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights, remove features with null geometry, and keep only necessary fields\n",
        "def apply_weights_and_filter(gdf):\n",
        "    print(\"Starting to calculate weights for each USACE office...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_usace_office_weight, axis=1)\n",
        "\n",
        "    # Filter out rows with null geometry\n",
        "    gdf_filtered = gdf[gdf.geometry.notnull()].copy()\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf_filtered[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated, null geometries filtered, and unnecessary fields removed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights, filter null geometries, and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_filter(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/USACE_Offices_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cc0A3Tjzc4X"
      },
      "source": [
        "## Healthcare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDHv99OWzc4Y"
      },
      "source": [
        "### Pharmacies\n",
        "**Dataset: Pharmacies -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::pharmacies-/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFMLW6D6zc4Y",
        "outputId": "a4c161a3-8986-4028-e56b-c4f863b2d2ab"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "ent_type_weights = {\n",
        "    '2': 1.000,  # Adjusted to match the example data ('ENT_TYPE': '2')\n",
        "    '3': 0.000\n",
        "}\n",
        "\n",
        "provid_11_weights = {\n",
        "    'WAL-MART PHARMACY': 0.300,\n",
        "    'CVS PHARMACY': 0.280,\n",
        "    'TARGET PHARMACY': 0.140,\n",
        "    'KROGER PHARMACY': 0.110,\n",
        "    'SAVON PHARMACY': 0.050,\n",
        "    'SAMS PHARMACY': 0.040,\n",
        "    'Other': 0.000  # Default case\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'ENT_TYPE': 0.500,\n",
        "    'PROVID_11': 0.500\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_pharmacy_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    ent_type_weight = ent_type_weights.get(row.get('ENT_TYPE', '3'), 0.0) * qualitative_importance_grades['ENT_TYPE']\n",
        "    provid_11_weight = provid_11_weights.get(row.get('PROVID_11', 'Other'), 0.0) * qualitative_importance_grades['PROVID_11']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = ent_type_weight + provid_11_weight\n",
        "\n",
        "    # Ensure the weight is a float and round to 4 decimal places\n",
        "    return round(total_weight, 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_pharmacies_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each pharmacy...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_pharmacy_weight, axis=1)\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry'\n",
        "    gdf = gdf[['geometry', 'Weight']]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_path):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_pharmacies_dataset(gdf)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "    print(\"Weighted dataset saved successfully.\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Pharmacies_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Output file path\n",
        "output_path = os.path.join(output_dir, \"Pharmacies_Weighted_cleaned.geojson\")\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jfs6i0szc4Y"
      },
      "source": [
        "### Dialysis Centers\n",
        "**Dataset: Dialysis Centers -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::dialysis-centers/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcbiylxnzc4Y",
        "outputId": "b1660472-37c4-4c57-846c-da044db53718"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    'Dialysis Center': 1.000\n",
        "}\n",
        "\n",
        "status_weights = {\n",
        "    'open': 0.999,\n",
        "    'unknown': 0.001\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'Type_': 0.500,\n",
        "    'Status': 0.500\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_dialysis_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = type_weights.get(row.get('Type_', 'Unknown'), 0.0) * qualitative_importance_grades['Type_']\n",
        "    status_weight = status_weights.get(row.get('Status', 'Unknown'), 0.0) * qualitative_importance_grades['Status']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = type_weight + status_weight\n",
        "\n",
        "    # Ensure the total weight is positive and rounded to 4 decimal places\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights, remove features with null geometry, and keep only necessary fields\n",
        "def apply_weights_and_filter(gdf):\n",
        "    print(\"Starting to calculate weights for each dialysis center...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_dialysis_weight, axis=1)\n",
        "\n",
        "    # Filter out rows with null geometry\n",
        "    gdf_filtered = gdf[gdf.geometry.notnull()].copy()\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' fields\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_stripped = gdf_filtered[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated, null geometries filtered, and unnecessary fields removed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_stripped\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights, filter null geometries, and strip unnecessary fields\n",
        "    gdf_weighted = apply_weights_and_filter(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Dialysis_Centers_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrzwE585zc4Z"
      },
      "source": [
        "### Hospitals\n",
        "**Dataset: Hospitals -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::hospitals/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snUsHUsHzc4Z",
        "outputId": "e2fa059e-b4b6-4eec-f5cb-b4e18dca31aa"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    'GENERAL ACUTE CARE': 0.500,\n",
        "    'CRITICAL ACCESS': 0.300,\n",
        "    'PSYCHIATRIC': 0.200,\n",
        "    'Other': 0.000  # This will handle other types not explicitly rated\n",
        "}\n",
        "\n",
        "status_weights = {\n",
        "    'OPEN': 0.950,\n",
        "    'CLOSED': 0.050\n",
        "}\n",
        "\n",
        "naics_weights = {\n",
        "    622110: 0.800,\n",
        "    622310: 0.100,\n",
        "    622210: 0.100\n",
        "}\n",
        "\n",
        "trauma_weights = {\n",
        "    'LEVEL I': 0.300,\n",
        "    'LEVEL II': 0.200,\n",
        "    'LEVEL III': 0.300,\n",
        "    'LEVEL IV': 0.300,\n",
        "    'TRH': 0.200,\n",
        "    'TRF': 0.200,\n",
        "    'CTH': 0.200,\n",
        "    'NOT AVAILABLE': 0.200\n",
        "}\n",
        "\n",
        "helipad_weights = {\n",
        "    'Y': 0.700,\n",
        "    'N': 0.300\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.300,\n",
        "    'STATUS': 0.250,\n",
        "    'NAICS_CODE': 0.200,\n",
        "    'TRAUMA': 0.150,\n",
        "    'HELIPAD': 0.100\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'BEDS': 1.000  # Since this is the only quantitative attribute, it gets full weight\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_hospital_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    type_weight = type_weights.get(row.get('TYPE', 'Other'), type_weights['Other']) * qualitative_importance_grades['TYPE']\n",
        "    status_weight = status_weights.get(row.get('STATUS', 'Other'), 0.0) * qualitative_importance_grades['STATUS']\n",
        "    naics_weight = naics_weights.get(row.get('NAICS_CODE', 0), 0.0) * qualitative_importance_grades['NAICS_CODE']\n",
        "    trauma_weight = trauma_weights.get(row.get('TRAUMA', 'NOT AVAILABLE'), 0.0) * qualitative_importance_grades['TRAUMA']\n",
        "    helipad_weight = helipad_weights.get(row.get('HELIPAD', 'N'), 0.0) * qualitative_importance_grades['HELIPAD']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    beds_weight = (row['BEDS'] / np.nanmax(row['BEDS'])) * quantitative_weights['BEDS'] if pd.notna(row['BEDS']) and row['BEDS'] > 0 else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = type_weight + status_weight + naics_weight + trauma_weight + helipad_weight + beds_weight\n",
        "\n",
        "    # Ensure the weight is non-negative, positive, and rounded to 4 decimal places\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights and remove extra fields\n",
        "\n",
        "def apply_weights_and_cleanup(gdf):\n",
        "    print(\"Starting to calculate weights for each hospital...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_hospital_weight, axis=1)\n",
        "\n",
        "    # Remove all fields except for 'Weight' and 'geometry'\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated and fields removed. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_path):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and clean up fields\n",
        "    gdf_weighted = apply_weights_and_cleanup(gdf)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Hospitals_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "output_file_name = \"Hospitals_Weighted.geojson\"\n",
        "output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOCLhgrlzc4a"
      },
      "source": [
        "### Public Health Departments\n",
        "**Dataset: Public Health Departments -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::public-health-departments/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2EOBDqhzc4a",
        "outputId": "b749ea7e-d1ef-4109-d42a-e76e942ff02c"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "hsipthemes_weights = {\n",
        "    'CRITICAL INFRASTRUCTURE, PDD-63; PUBLIC HEALTH; HEALTH SERVICES; PUBLIC HEALTH OFFICES, STATE AND LOCAL': 1.000\n",
        "}\n",
        "\n",
        "govt_level_weights = {\n",
        "    'LOCAL': 0.800,\n",
        "    'STATE': 0.150,\n",
        "    'TRIBAL': 0.030,\n",
        "    'OTHER': 0.020\n",
        "}\n",
        "\n",
        "st_vendor_weights = {\n",
        "    'NAVTEQ': 0.990,\n",
        "    'TGS': 0.010,\n",
        "    'GU_GOV': 0.000,\n",
        "    'MP_GOV': 0.000,\n",
        "    'AS_GOV': 0.000\n",
        "}\n",
        "\n",
        "st_version_weights = {\n",
        "    '2009Q2': 1.000,\n",
        "    '2006': 0.000,\n",
        "    '2007': 0.000,\n",
        "    '1990': 0.000\n",
        "}\n",
        "\n",
        "sdr_weights = {\n",
        "    'YES': 0.950,\n",
        "    'NO': 0.050,\n",
        "    'Unspecified': 0.000\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'HSIPTHEMES': 0.300,\n",
        "    'GOVT_LEVEL': 0.250,\n",
        "    'ST_VENDOR': 0.200,\n",
        "    'ST_VERSION': 0.150,\n",
        "    'SDR': 0.100\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'TOT_STAFF': 1.000  # Adjusting to give some impact for this quantitative measure\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_health_dept_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    hsipthemes_weight = hsipthemes_weights.get(row.get('HSIPTHEMES', ''), 0.0) * qualitative_importance_grades['HSIPTHEMES']\n",
        "    govt_level_weight = govt_level_weights.get(row.get('GOVT_LEVEL', 'OTHER'), 0.0) * qualitative_importance_grades['GOVT_LEVEL']\n",
        "    st_vendor_weight = st_vendor_weights.get(row.get('ST_VENDOR', ''), 0.0) * qualitative_importance_grades['ST_VENDOR']\n",
        "    st_version_weight = st_version_weights.get(row.get('ST_VERSION', ''), 0.0) * qualitative_importance_grades['ST_VERSION']\n",
        "    sdr_weight = sdr_weights.get(row.get('SDR', 'Unspecified'), 0.0) * qualitative_importance_grades['SDR']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    tot_staff = row.get('TOT_STAFF', 0) if pd.notna(row.get('TOT_STAFF')) else 0\n",
        "    max_tot_staff = max(1, tot_staff)  # Prevent division by zero\n",
        "    tot_staff_weight = (tot_staff / max_tot_staff) * quantitative_weights['TOT_STAFF'] if max_tot_staff > 0 else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        hsipthemes_weight +\n",
        "        govt_level_weight +\n",
        "        st_vendor_weight +\n",
        "        st_version_weight +\n",
        "        sdr_weight +\n",
        "        tot_staff_weight\n",
        "    )\n",
        "\n",
        "    # Ensure the weight is positive, non-null, and rounded to 4 decimal places\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and filter unnecessary fields\n",
        "def apply_weights_to_health_dept_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each public health department...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_health_dept_weight, axis=1)\n",
        "\n",
        "    # Keep only relevant fields (in this case, 'Weight' and 'geometry')\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_filtered = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated and unnecessary fields removed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_filtered\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and filter unnecessary fields\n",
        "    gdf_weighted = apply_weights_to_health_dept_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Public_Health_Departments_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-5NlwGlXEkt"
      },
      "source": [
        "### Urgent Care Facilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8SgKptWXIyc",
        "outputId": "318c299c-7e5d-4236-f1cc-9712517f8b6e"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "hsipthemes_weights = {\n",
        "    'CRITICAL INFRASTRUCTURE, PDD-63; PUBLIC HEALTH; PRIMARY CARE FACILITIES (INCLUDING HOSPITALS); AMBULATORY SURGICAL FACILITIES': 1.000\n",
        "}\n",
        "\n",
        "st_vendor_weights = {\n",
        "    'NAVTEQ': 0.990,\n",
        "    'TGS': 0.010,\n",
        "    'GU_GOV': 0.000,\n",
        "    'MP_GOV': 0.000,\n",
        "    'AS_GOV': 0.000\n",
        "}\n",
        "\n",
        "st_version_weights = {\n",
        "    '2008Q1': 1.000,\n",
        "    '2006': 0.000,\n",
        "    '2007': 0.000,\n",
        "    '1990': 0.000\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'HSIPTHEMES': 0.500,\n",
        "    'ST_VENDOR': 0.300,\n",
        "    'ST_VERSION': 0.200\n",
        "}\n",
        "\n",
        "# No quantitative attributes in this case, but you can adjust if needed\n",
        "quantitative_weights = {}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_urgent_care_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    hsipthemes_weight = hsipthemes_weights.get(row.get('HSIPTHEMES', ''), 0.0) * qualitative_importance_grades['HSIPTHEMES']\n",
        "    st_vendor_weight = st_vendor_weights.get(row.get('ST_VENDOR', ''), 0.0) * qualitative_importance_grades['ST_VENDOR']\n",
        "    st_version_weight = st_version_weights.get(row.get('ST_VERSION', ''), 0.0) * qualitative_importance_grades['ST_VERSION']\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        hsipthemes_weight +\n",
        "        st_vendor_weight +\n",
        "        st_version_weight\n",
        "    )\n",
        "\n",
        "    # Ensure the weight is positive, non-null, and rounded to 4 decimal places\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and filter unnecessary fields\n",
        "def apply_weights_to_urgent_care_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each urgent care facility...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_urgent_care_weight, axis=1)\n",
        "\n",
        "    # Keep only relevant fields (in this case, 'Weight' and 'geometry')\n",
        "    fields_to_keep = ['Weight', 'geometry']\n",
        "    gdf_filtered = gdf[fields_to_keep]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated and unnecessary fields removed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf_filtered\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights and filter unnecessary fields\n",
        "    gdf_weighted = apply_weights_to_urgent_care_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Urgent_Care_Facilities_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wImgMYYMXJPt"
      },
      "source": [
        "### VA Facilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc0TcbW8XK4D"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights for NAICSCODE\n",
        "naicscode_weights = {\n",
        "    '62149': 0.700,   # Outpatient Care Centers\n",
        "    '622110': 0.250,  # General Medical and Surgical Hospitals\n",
        "    '623110': 0.040,  # Nursing Care Facilities\n",
        "    '622310': 0.010   # Specialty Hospitals\n",
        "}\n",
        "\n",
        "# Define qualitative attribute weights for PRIM_SVC\n",
        "prims_svc_weights = {\n",
        "    'CBOC': 0.500,  # Community Based Outpatient Clinic\n",
        "    'VCTR': 0.300,  # Veterans Center\n",
        "    'VAMC': 0.150,  # Veterans Affairs Medical Center\n",
        "    'VANH': 0.030,  # Veterans Affairs Nursing Home\n",
        "    'IOC': 0.015,   # Independent Outpatient Clinic\n",
        "    'RRTP': 0.005   # Residential Rehabilitation Treatment Program\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'NAICSCODE': 0.600,\n",
        "    'PRIM_SVC': 0.400\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_va_facility_weight(row):\n",
        "    # Ensure no missing values by using defaults\n",
        "    naicscode_weight = naicscode_weights.get(row.get('NAICSCODE', ''), 0.0) * qualitative_importance_grades['NAICSCODE']\n",
        "    prim_svc_weight = prims_svc_weights.get(row.get('PRIM_SVC', ''), 0.0) * qualitative_importance_grades['PRIM_SVC']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = naicscode_weight + prim_svc_weight\n",
        "\n",
        "    # Ensure the weight is positive, non-null, and rounded to 4 decimal places\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_va_facilities_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each VA Medical Facility...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_va_facility_weight, axis=1)\n",
        "\n",
        "    # Remove all fields except 'Weight' and 'geometry'\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_va_facilities_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/VA_Medical_Facilities_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaFNUM8Kzc4a"
      },
      "source": [
        "## Industrial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vao1X7qBzc4b"
      },
      "source": [
        "### Fortune 500 Headquarters\n",
        "**Dataset: Fortune 500 Corporate Headquarters -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::fortune-500-corporate-headquarters/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkxBsSHrzc4b"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define the weights for quantitative attributes\n",
        "quantitative_weights = {\n",
        "    'RANK': 0.300,       # Lower ranks (e.g., 1) are more important\n",
        "    'EMPLOYEES': 0.250,  # More employees indicate a larger, more significant company\n",
        "    'REVENUES': 0.300,   # Higher revenue is more critical\n",
        "    'PROFIT': 0.150      # Higher profit indicates financial success and importance\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_fortune500_weight(row, max_employees, max_revenues, max_profit):\n",
        "    # Ensure all fields are present and non-null\n",
        "    rank = row.get('RANK', 0) if pd.notna(row.get('RANK', 0)) else 0\n",
        "    employees = row.get('EMPLOYEES', 0) if pd.notna(row.get('EMPLOYEES', 0)) else 0\n",
        "    revenues = row.get('REVENUES', 0) if pd.notna(row.get('REVENUES', 0)) else 0\n",
        "    profit = row.get('PROFIT', 0) if pd.notna(row.get('PROFIT', 0)) else 0\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    rank_weight = ((500 - rank) / 499) * quantitative_weights['RANK'] if rank > 0 else 0\n",
        "    employees_weight = (employees / max(1, max_employees)) * quantitative_weights['EMPLOYEES'] if employees > 0 else 0\n",
        "    revenues_weight = (revenues / max(1, max_revenues)) * quantitative_weights['REVENUES'] if revenues > 0 else 0\n",
        "    profit_weight = (profit / max(1, max_profit)) * quantitative_weights['PROFIT'] if profit > 0 else 0\n",
        "\n",
        "    # Combine quantitative weights\n",
        "    total_weight = rank_weight + employees_weight + revenues_weight + profit_weight\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_fortune500_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each company...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get the maximum values for scaling\n",
        "    max_employees = np.nanmax(gdf['EMPLOYEES'])\n",
        "    max_revenues = np.nanmax(gdf['REVENUES'])\n",
        "    max_profit = np.nanmax(gdf['PROFIT'])\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_fortune500_weight, axis=1, args=(max_employees, max_revenues, max_profit))\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_fortune500_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Fortune_500_HQ_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjibIWNizc4b"
      },
      "source": [
        "### Manufacturing\n",
        "**Dataset: General Manufacturing Facilities -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::general-manufacturing-facilities/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_AN-IyTzc4b"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "product_weights = {\n",
        "    'NEWSPAPER PUBLISHING': 0.400,\n",
        "    'COMMERCIAL PRINTING': 0.350,\n",
        "    'READY-MIXED CONCRETE': 0.250\n",
        "}\n",
        "\n",
        "sic_weights = {\n",
        "    3089: 0.400,\n",
        "    3599: 0.350,\n",
        "    2759: 0.250\n",
        "}\n",
        "\n",
        "sic2_weights = {\n",
        "    3599: 0.450,\n",
        "    3089: 0.350,\n",
        "    3499: 0.200\n",
        "}\n",
        "\n",
        "sic3_weights = {\n",
        "    3599: 0.600,\n",
        "    3089: 0.400\n",
        "}\n",
        "\n",
        "sic4_weights = {\n",
        "    3: 0.500,\n",
        "    1: 0.300,\n",
        "    3599: 0.200\n",
        "}\n",
        "\n",
        "naics_weights = {\n",
        "    332710: 0.600,\n",
        "    323119: 0.400\n",
        "}\n",
        "\n",
        "naics_descr_weights = {\n",
        "    'NOT AVAILABLE': 0.200,\n",
        "    'MACHINE SHOPS': 0.400,\n",
        "    'OTHER COMMERCIAL PRINTING': 0.400\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'PRODUCT': 0.200,\n",
        "    'SIC': 0.150,\n",
        "    'SIC2': 0.150,\n",
        "    'SIC3': 0.100,\n",
        "    'SIC4': 0.100,\n",
        "    'NAICS': 0.150,\n",
        "    'NAICSDESCR': 0.150\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'EMP': 0.500  # The number of employees is very important\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_manufacturing_weight(row, max_emp):\n",
        "    # Calculate qualitative weights\n",
        "    product_weight = product_weights.get(row.get('PRODUCT', ''), 0.0) * qualitative_importance_grades['PRODUCT']\n",
        "    sic_weight = sic_weights.get(row.get('SIC', ''), 0.0) * qualitative_importance_grades['SIC']\n",
        "    sic2_weight = sic2_weights.get(row.get('SIC2', ''), 0.0) * qualitative_importance_grades['SIC2']\n",
        "    sic3_weight = sic3_weights.get(row.get('SIC3', ''), 0.0) * qualitative_importance_grades['SIC3']\n",
        "    sic4_weight = sic4_weights.get(row.get('SIC4', ''), 0.0) * qualitative_importance_grades['SIC4']\n",
        "    naics_weight = naics_weights.get(row.get('NAICS', ''), 0.0) * qualitative_importance_grades['NAICS']\n",
        "    naics_descr_weight = naics_descr_weights.get(row.get('NAICSDESCR', ''), 0.0) * qualitative_importance_grades['NAICSDESCR']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    emp = row.get('EMP', 0) if pd.notna(row.get('EMP', 0)) else 0\n",
        "    emp_weight = (emp / max_emp) * quantitative_weights['EMP'] if emp > 0 else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        product_weight +\n",
        "        sic_weight +\n",
        "        sic2_weight +\n",
        "        sic3_weight +\n",
        "        sic4_weight +\n",
        "        naics_weight +\n",
        "        naics_descr_weight +\n",
        "        emp_weight\n",
        "    )\n",
        "\n",
        "    # Ensure the weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "\n",
        "def apply_weights_to_manufacturing_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each manufacturing facility...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get the maximum EMP value for scaling\n",
        "    max_emp = max(1, gdf['EMP'].max())  # Prevent division by zero\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_manufacturing_weight, axis=1, args=(max_emp,))\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_manufacturing_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Manufacturing_Facilities_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trj5UTvwzc4c"
      },
      "source": [
        "## Military"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_w6lr5-zc4c"
      },
      "source": [
        "### Military Installations\n",
        "**Dataset: Military Installations, Ranges, and Training Areas (MIRTA) DoD Sites - Points -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::military-installations-ranges-and-training-areas-mirta-dod-sites-points/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Djjw_ZY5zc4c",
        "outputId": "3e596709-5e95-4567-c27b-33afd81932a5"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "site_reporting_component_weights = {\n",
        "    'usn': 0.250,\n",
        "    'armyNationalGuard': 0.230,\n",
        "    'usaf': 0.200,\n",
        "    'usa': 0.150,\n",
        "    'airNationalGuard': 0.100,\n",
        "    'Summary for Smaller Components': 0.070\n",
        "}\n",
        "\n",
        "site_operational_status_weights = {\n",
        "    'act': 5.000,\n",
        "    'clsd': 0.050,\n",
        "    'care': 0.040,\n",
        "    'Summary for Other Statuses': 0.010\n",
        "}\n",
        "\n",
        "is_joint_base_weights = {\n",
        "    'Yes': 0.600,\n",
        "    'No': 0.400\n",
        "}\n",
        "\n",
        "is_firrma_site_weights = {\n",
        "    'Yes': 0.600,\n",
        "    'No': 0.400\n",
        "}\n",
        "\n",
        "is_cui_weights = {\n",
        "    'Yes': 0.300,\n",
        "    'No': 0.700\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'siteReportingComponent': 0.300,\n",
        "    'siteOperationalStatus': 0.400,\n",
        "    'isJointBase': 0.100,\n",
        "    'isFirrmaSite': 0.100,\n",
        "    'isCui': 0.100\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_military_installation_weight(row):\n",
        "    # Calculate qualitative weights, setting missing fields to 0\n",
        "    site_reporting_weight = site_reporting_component_weights.get(row.get('siteReportingComponent', ''), 0.0) * qualitative_importance_grades['siteReportingComponent']\n",
        "    operational_status_weight = site_operational_status_weights.get(row.get('siteOperationalStatus', ''), 0.0) * qualitative_importance_grades['siteOperationalStatus']\n",
        "    joint_base_weight = is_joint_base_weights.get(row.get('isJointBase', ''), 0.0) * qualitative_importance_grades['isJointBase']\n",
        "    firrma_site_weight = is_firrma_site_weights.get(row.get('isFirrmaSite', ''), 0.0) * qualitative_importance_grades['isFirrmaSite']\n",
        "    cui_weight = is_cui_weights.get(row.get('isCui', ''), 0.0) * qualitative_importance_grades['isCui']\n",
        "\n",
        "    # Combine qualitative weights and round to 4 decimal places\n",
        "    total_weight = round((\n",
        "        site_reporting_weight +\n",
        "        operational_status_weight +\n",
        "        joint_base_weight +\n",
        "        firrma_site_weight +\n",
        "        cui_weight\n",
        "    ), 4)\n",
        "\n",
        "    # Ensure the weight is positive\n",
        "    return max(total_weight, 0.0)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "\n",
        "def apply_weights_to_military_installations_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each military installation...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_military_installation_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf[['Weight', 'geometry']]  # Keep only 'Weight' and 'geometry' columns\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_military_installations_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Military_Installations_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgfXMMdgzc4c"
      },
      "source": [
        "## Mines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc53a_gGzc4c"
      },
      "source": [
        "### Agricultural Minerals Operations\n",
        "**Dataset: Agricultural Minerals Operations -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::agricultural-minerals-operations/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STvGJw2Xzc4d",
        "outputId": "65a12055-1fc7-4201-cb0f-fd59914f4ee8"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "commodity_weights = {\n",
        "    'SULFUR': 0.400,\n",
        "    'PEAT': 0.200,\n",
        "    'PHOSPHATE': 0.150,\n",
        "    'VERMICULITE': 0.100,\n",
        "    'MAGNESIUM COMPOUNDS': 0.075,\n",
        "    'POTASH': 0.075\n",
        "}\n",
        "\n",
        "plant_min_weights = {\n",
        "    'P': 0.600,   # Phosphate\n",
        "    'M': 0.260,   # Magnesium\n",
        "    'M/P': 0.140  # Magnesium/Phosphate\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'COMMODITY': 0.600,\n",
        "    'PLANT_MIN': 0.400\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_mineral_weight(row):\n",
        "    # Ensure all fields are present and non-null\n",
        "    commodity = row.get('COMMODITY', '') if pd.notna(row.get('COMMODITY')) else ''\n",
        "    plant_min = row.get('PLANT_MIN', '') if pd.notna(row.get('PLANT_MIN')) else ''\n",
        "\n",
        "    # Calculate qualitative weights\n",
        "    commodity_weight = commodity_weights.get(commodity, 0.0) * qualitative_importance_grades['COMMODITY']\n",
        "    plant_min_weight = plant_min_weights.get(plant_min, 0.0) * qualitative_importance_grades['PLANT_MIN']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = commodity_weight + plant_min_weight\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_mineral_operations_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each operation...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_mineral_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_mineral_operations_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Agricultural_Minerals_Operations_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMMl_D9rzc4d"
      },
      "source": [
        "### Construction Minerals Operations\n",
        "**Dataset: Construction Minerals Operations -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::construction-minerals-operations/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yU06GJdzc4d",
        "outputId": "aba41155-dd88-4ee6-96a5-0e58e9caf3f3"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights for COMMODITY\n",
        "commodity_weights = {\n",
        "    \"COMMON CLAY AND SHALE\": 0.350,\n",
        "    \"DIMENSION STONE\": 0.250,\n",
        "    \"CEMENT\": 0.200,\n",
        "    \"PERLITE\": 0.100,\n",
        "    \"GYPSUM\": 0.050,\n",
        "    \"BALL CLAY\": 0.030,\n",
        "    \"MICA\": 0.010,\n",
        "    \"PUMICE\": 0.010\n",
        "}\n",
        "\n",
        "# Define qualitative attribute weights for PLANT_MIN\n",
        "plant_min_weights = {\n",
        "    \"M/P\": 0.400,\n",
        "    \"M\": 0.350,\n",
        "    \"P\": 0.250\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'COMMODITY': 0.650,\n",
        "    'PLANT_MIN': 0.350\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_construction_minerals_weight(row):\n",
        "    # Handle missing or null values and ensure all fields have valid data\n",
        "    commodity_weight = commodity_weights.get(row.get('COMMODITY', ''), 0.0) * qualitative_importance_grades['COMMODITY']\n",
        "    plant_min_weight = plant_min_weights.get(row.get('PLANT_MIN', ''), 0.0) * qualitative_importance_grades['PLANT_MIN']\n",
        "\n",
        "    # Combine qualitative weights (no quantitative weights provided)\n",
        "    total_weight = commodity_weight + plant_min_weight\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_construction_minerals_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each construction mineral operation...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_construction_minerals_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_construction_minerals_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Construction_Minerals_Operations_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnoYyhQkzc4d"
      },
      "source": [
        "### Ferrous Metal Mines\n",
        "**Dataset: Ferrous Metal Mines -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::ferrous-metal-mines/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUfpeJ40zc4d",
        "outputId": "1da297cc-02ff-4cf8-a937-3a8342a11ae4"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "commodity_weights = {\n",
        "    'Iron': 0.350,\n",
        "    'Molybdenum': 0.300,\n",
        "    'Cobalt': 0.200,\n",
        "    'Rhenium': 0.100,\n",
        "    'Nickel': 0.050\n",
        "}\n",
        "\n",
        "plant_mine_weights = {\n",
        "    'M/P': 0.600,\n",
        "    'M': 0.300,\n",
        "    'P': 0.100\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'COMMODITY': 0.700,\n",
        "    'PLANT_MINE': 0.300\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_mine_weight(row):\n",
        "    # Ensure all fields are present and non-null\n",
        "    commodity = row.get('COMMODITY', '') if pd.notna(row.get('COMMODITY', '')) else ''\n",
        "    plant_mine = row.get('PLANT_MINE', '') if pd.notna(row.get('PLANT_MINE', '')) else ''\n",
        "\n",
        "    # Calculate qualitative weights\n",
        "    commodity_weight = commodity_weights.get(commodity, 0.0) * qualitative_importance_grades['COMMODITY']\n",
        "    plant_mine_weight = plant_mine_weights.get(plant_mine, 0.0) * qualitative_importance_grades['PLANT_MINE']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = commodity_weight + plant_mine_weight\n",
        "\n",
        "    # Ensure the weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_mines_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each mine...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_mine_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_mines_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Ferrous_Metal_Mines_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhhU6Y7Kzc4e"
      },
      "source": [
        "### Ferrous Metal Processing Plants\n",
        "**Dataset: Ferrous Metal Processing Plants -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::ferrous-metal-processing-plants/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrrX5Zrlzc4e",
        "outputId": "8f3b756a-8c85-43f7-855a-55da7c0fbae9"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "commodity_weights = {\n",
        "    'Iron': 0.300,\n",
        "    'Molybdenum': 0.250,\n",
        "    'Silicon': 0.200,\n",
        "    'Cobalt': 0.150,\n",
        "    'Rhenium': 0.050,\n",
        "    'Columbium and(or) tantalum': 0.030,\n",
        "    'Nickel': 0.030,\n",
        "    'Tungsten': 0.030,\n",
        "    'Chromium': 0.020,\n",
        "    'Columbium': 0.020,\n",
        "    'Manganese': 0.020\n",
        "}\n",
        "\n",
        "plant_mine_weights = {\n",
        "    'P': 0.500,\n",
        "    'M/P': 0.350,\n",
        "    'M': 0.150\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'COMMODITY': 0.650,\n",
        "    'PLANT_MINE': 0.350\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_ferrous_plant_weight(row):\n",
        "    # Ensure all fields are present and non-null\n",
        "    commodity = row.get('COMMODITY', '') if pd.notna(row.get('COMMODITY', '')) else ''\n",
        "    plant_mine = row.get('PLANT_MINE', '') if pd.notna(row.get('PLANT_MINE', '')) else ''\n",
        "\n",
        "    # Calculate qualitative weights\n",
        "    commodity_weight = commodity_weights.get(commodity, 0.0) * qualitative_importance_grades['COMMODITY']\n",
        "    plant_mine_weight = plant_mine_weights.get(plant_mine, 0.0) * qualitative_importance_grades['PLANT_MINE']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = commodity_weight + plant_mine_weight\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_ferrous_plants_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each ferrous metal processing plant...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_ferrous_plant_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_ferrous_plants_dataset(gdf)\n",
        "\n",
        "    # Drop all fields except 'Weight' and 'geometry'\n",
        "    gdf_weighted = gdf_weighted[['Weight', 'geometry']]\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Ferrous_Metal_Processing_Plants_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XIIF2C_zc4e"
      },
      "source": [
        "### Mines and Mineral Resources\n",
        "**Dataset: Mines and Mineral Resources -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::mines-and-mineral-resources/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVo4EC-Xzc4e",
        "outputId": "ca7fcbc1-c505-4af7-c8f0-81d77d2f6e6c"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "naicscode_weights = {\n",
        "    '212111': 0.300,\n",
        "    '212112': 0.250,\n",
        "    '212311': 0.200,\n",
        "    '212325': 0.150,\n",
        "    '212399': 0.100\n",
        "}\n",
        "\n",
        "mine_type_weights = {\n",
        "    '12': 0.250,\n",
        "    '11': 0.200,\n",
        "    '06': 0.150,\n",
        "    '05': 0.150,\n",
        "    '04': 0.100,\n",
        "    'Others': 0.150\n",
        "}\n",
        "\n",
        "stat_code_weights = {\n",
        "    'A': 0.500,\n",
        "    '1': 0.250,\n",
        "    '2': 0.150,\n",
        "    'Others': 0.100\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'NAICSCODE': 0.400,\n",
        "    'MINE_TYPE': 0.350,\n",
        "    'STAT_CODE': 0.250\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_mine_weight(row):\n",
        "    # Ensure all fields are present and non-null\n",
        "    naicscode = row.get('NAICSCODE', 'Unknown')\n",
        "    mine_type = row.get('MINE_TYPE', 'Others')\n",
        "    stat_code = row.get('STAT_CODE', 'Others')\n",
        "\n",
        "    # Calculate qualitative weights\n",
        "    naicscode_weight = naicscode_weights.get(naicscode, 0.0) * qualitative_importance_grades['NAICSCODE']\n",
        "    mine_type_weight = mine_type_weights.get(mine_type, mine_type_weights['Others']) * qualitative_importance_grades['MINE_TYPE']\n",
        "    stat_code_weight = stat_code_weights.get(stat_code, stat_code_weights['Others']) * qualitative_importance_grades['STAT_CODE']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = naicscode_weight + mine_type_weight + stat_code_weight\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_mines_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each mining operation...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_mine_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_mines_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Mines_and_Mineral_Resources_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4osvBjHzc4e"
      },
      "source": [
        "### Nonferrous Metal Mines\n",
        "**Dataset: Nonferrous Metal Mines -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::nonferrous-metal-mines/explore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNvdLBjrzc4f",
        "outputId": "1ec0ac96-8ef7-4363-fd99-2712d5a4fc5c"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "commodity_weights = {\n",
        "    'Silver': 0.300,\n",
        "    'Gold': 0.250,\n",
        "    'Copper': 0.200,\n",
        "    'Zinc': 0.100,\n",
        "    'Lead': 0.080,\n",
        "    'Beryllium': 0.035,\n",
        "    'Germanium': 0.035\n",
        "}\n",
        "\n",
        "plant_mine_weights = {\n",
        "    'M': 0.600,  # Mine\n",
        "    'M/P': 0.250,  # Mine/Processing Plant\n",
        "    'P': 0.150  # Processing Plant\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'COMMODITY': 0.700,\n",
        "    'PLANT_MINE': 0.300\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_mine_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    commodity_weight = commodity_weights.get(row['COMMODITY'], 0.0) * qualitative_importance_grades['COMMODITY']\n",
        "    plant_mine_weight = plant_mine_weights.get(row['PLANT_MINE'], 0.0) * qualitative_importance_grades['PLANT_MINE']\n",
        "\n",
        "    # Combine qualitative weights (since no quantitative data is provided)\n",
        "    total_weight = commodity_weight + plant_mine_weight\n",
        "\n",
        "    # Ensure the weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset and remove fields\n",
        "\n",
        "def apply_weights_to_mines_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each mine...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize progress bar\n",
        "    progress_bar = tqdm(total=len(gdf), desc=\"Processing rows\", unit=\"rows\")\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_mine_weight, axis=1)\n",
        "\n",
        "    progress_bar.update(1)\n",
        "    progress_bar.close()\n",
        "\n",
        "    # Drop all columns except 'Weight' and 'geometry'\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_mines_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Nonferrous_Metal_Mines_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu7LIG_bzc4f"
      },
      "source": [
        "### Nonferrous Metal Processing Plants\n",
        "**Dataset: Nonferrous Metal Processing Plants -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::nonferrous-metal-processing-plants/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pktllhtfzc4f",
        "outputId": "1fe5115a-dd03-434a-c2b3-665546e8a836"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "commodity_weights = {\n",
        "    'ALUMINUM': 0.300,\n",
        "    'COPPER': 0.250,\n",
        "    'IRON': 0.150,\n",
        "    'SILVER': 0.120,\n",
        "    'GOLD': 0.080,\n",
        "    'RHENIUM': 0.030,\n",
        "    'ANTIMONY': 0.020,\n",
        "    'LEAD': 0.020,\n",
        "    'TITANIUM METAL': 0.020,\n",
        "    'ZINC': 0.020,\n",
        "    'BERYLLIUM': 0.010,\n",
        "    'CADMIUM': 0.010,\n",
        "    'MAGNESIUM METAL': 0.010,\n",
        "    'SELENIUM': 0.010,\n",
        "    'STRONTIUM': 0.010\n",
        "}\n",
        "\n",
        "plant_min_weights = {\n",
        "    'P': 0.630,\n",
        "    'M/P': 0.370\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'COMMODITY': 0.700,\n",
        "    'PLANT_MIN': 0.300\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_metal_processing_plant_weight(row):\n",
        "    # Ensure all fields are present and non-null\n",
        "    commodity = row.get('COMMODITY', 'Other') if pd.notna(row.get('COMMODITY', 'Other')) else 'Other'\n",
        "    plant_min = row.get('PLANT_MIN', 'Other') if pd.notna(row.get('PLANT_MIN', 'Other')) else 'Other'\n",
        "\n",
        "    # Calculate qualitative weights\n",
        "    commodity_weight = commodity_weights.get(commodity, 0.0) * qualitative_importance_grades['COMMODITY']\n",
        "    plant_min_weight = plant_min_weights.get(plant_min, 0.0) * qualitative_importance_grades['PLANT_MIN']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = commodity_weight + plant_min_weight\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "\n",
        "def apply_weights_to_metal_processing_plants_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each plant...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_metal_processing_plant_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_metal_processing_plants_dataset(gdf)\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf_weighted = gdf_weighted[['Weight', 'geometry']]\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Nonferrous_Metal_Processing_Plants_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlpf11ouzc4f"
      },
      "source": [
        "### Refractory Abrasive and Other Industrial Mineral Operations\n",
        "**Dataset: Refractory Abrasive and Other Industrial Mineral Operations -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::refractory-abrasive-and-other-industrial-mineral-operations/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loPLyO_1zc4f",
        "outputId": "6a40a738-27a7-41d4-ed6b-6e4ac8eca0f0"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "commodity_weights = {\n",
        "    'Gemstones': 0.250,\n",
        "    'Bentonite': 0.200,\n",
        "    'Talc': 0.150,\n",
        "    'Zeolites': 0.100,\n",
        "    'Silica': 0.080,\n",
        "    'Feldspar': 0.070,\n",
        "    'Diatomite': 0.050,\n",
        "    'Boron': 0.040,\n",
        "    'Pyrophyllite': 0.020,\n",
        "    'Zircon': 0.020,\n",
        "    'Garnet': 0.015,\n",
        "    'Olivine': 0.015,\n",
        "    'Wollastonite': 0.015,\n",
        "    'Kyanite': 0.010,\n",
        "    'Trona': 0.010\n",
        "}\n",
        "\n",
        "plant_min_weights = {\n",
        "    'M': 0.600,\n",
        "    'M/P': 0.250,\n",
        "    'P': 0.150\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'COMMODITY': 0.700,\n",
        "    'PLANT_MIN': 0.300\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_mineral_operations_weight(row):\n",
        "    # Ensure all fields are present and non-null\n",
        "    commodity = row.get('COMMODITY', '') if pd.notna(row.get('COMMODITY', '')) else ''\n",
        "    plant_min = row.get('PLANT_MIN', '') if pd.notna(row.get('PLANT_MIN', '')) else ''\n",
        "\n",
        "    # Calculate qualitative weights\n",
        "    commodity_weight = commodity_weights.get(commodity, 0.0) * qualitative_importance_grades['COMMODITY']\n",
        "    plant_min_weight = plant_min_weights.get(plant_min, 0.0) * qualitative_importance_grades['PLANT_MIN']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = commodity_weight + plant_min_weight\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_mineral_operations_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each operation...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_mineral_operations_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_mineral_operations_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Industrial_Mineral_Operations_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLJdWuwjzc4g"
      },
      "source": [
        "### Sand and Gravel Operations\n",
        "**Dataset: Sand and Gravel Operations -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::sand-and-gravel-operations-1/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7tA39h7zc4g",
        "outputId": "b65791fe-3a6f-4fb3-b259-4ed66a44d35d"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "commodity_weights = {\n",
        "    'SAND & GRAVEL': 1.000\n",
        "}\n",
        "\n",
        "oper_type_weights = {\n",
        "    'OPEN PIT': 0.600,\n",
        "    'DREDGE': 0.300,\n",
        "    'NOT AVAILABLE': 0.050,\n",
        "    'SALES YARD': 0.050\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'COMMODITY': 0.500,\n",
        "    'OPER_TYPE': 0.500\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_sand_gravel_weight(row):\n",
        "    # Ensure that missing values are handled as 0\n",
        "    commodity_weight = commodity_weights.get(row.get('COMMODITY', 'NOT AVAILABLE'), 0.0) * qualitative_importance_grades['COMMODITY']\n",
        "    oper_type_weight = oper_type_weights.get(row.get('OPER_TYPE', 'NOT AVAILABLE'), 0.0) * qualitative_importance_grades['OPER_TYPE']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = commodity_weight + oper_type_weight\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "\n",
        "def apply_weights_to_sand_gravel_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each operation...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_sand_gravel_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_sand_gravel_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Sand_and_Gravel_Operations_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7vs_nEAzc4g"
      },
      "source": [
        "### Uranium and Vanadium Deposits\n",
        "**Dataset: Uranium and Vanadium Deposits -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::uranium-and-vanadium-deposits/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_Z--8cpzc4g",
        "outputId": "4f91df50-cc7c-4154-ca01-8da10ace64d0"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "com_major_weights = {\n",
        "    \"Uranium\": 0.500,\n",
        "    \"Uranium, Vanadium\": 0.200,\n",
        "    \"Not Available\": 0.150,\n",
        "    \"Other\": 0.150\n",
        "}\n",
        "\n",
        "dev_stat_weights = {\n",
        "    \"Past Producer\": 0.400,\n",
        "    \"Occurrence\": 0.300,\n",
        "    \"Producer\": 0.200,\n",
        "    \"Prospect\": 0.100,\n",
        "    \"Other\": 0.000\n",
        "}\n",
        "\n",
        "ore_weights = {\n",
        "    \"Uraninite\": 0.600,\n",
        "    \"Uraninite, Uranophane\": 0.200,\n",
        "    \"Other\": 0.200\n",
        "}\n",
        "\n",
        "orebody_fm_weights = {\n",
        "    \"Irregular\": 0.300,\n",
        "    \"Tabular\": 0.250,\n",
        "    \"Not Available\": 0.200,\n",
        "    \"Other\": 0.250\n",
        "}\n",
        "\n",
        "work_type_weights = {\n",
        "    \"Underground\": 0.400,\n",
        "    \"Surface\": 0.300,\n",
        "    \"Not Available\": 0.200,\n",
        "    \"Surface/Underground\": 0.100,\n",
        "    \"Other\": 0.000\n",
        "}\n",
        "\n",
        "model_weights = {\n",
        "    \"Not Available\": 0.800,\n",
        "    \"Other\": 0.200\n",
        "}\n",
        "\n",
        "hrock_unit_weights = {\n",
        "    \"Chinle Formation, Moss Back Member\": 0.300,\n",
        "    \"Mount Holly Complex\": 0.250,\n",
        "    \"Not Available\": 0.200,\n",
        "    \"Other\": 0.250\n",
        "}\n",
        "\n",
        "hrock_type_weights = {\n",
        "    \"Sandstone\": 0.300,\n",
        "    \"Mudstone, Sandstone\": 0.250,\n",
        "    \"Not Available\": 0.200,\n",
        "    \"Other\": 0.250\n",
        "}\n",
        "\n",
        "arock_unit_weights = {\n",
        "    \"Not Available\": 0.900,\n",
        "    \"Other\": 0.100\n",
        "}\n",
        "\n",
        "arock_type_weights = {\n",
        "    \"Not Available\": 0.900,\n",
        "    \"Other\": 0.100\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'COM_MAJOR': 0.150,\n",
        "    'DEV_STAT': 0.150,\n",
        "    'ORE': 0.200,\n",
        "    'OREBODY_FM': 0.100,\n",
        "    'WORK_TYPE': 0.100,\n",
        "    'MODEL': 0.050,\n",
        "    'HROCK_UNIT': 0.050,\n",
        "    'HROCK_TYPE': 0.050,\n",
        "    'AROCK_UNIT': 0.050,\n",
        "    'AROCK_TYPE': 0.050\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_deposit_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    com_major_weight = com_major_weights.get(row.get('COM_MAJOR', 'Other'), com_major_weights['Other']) * qualitative_importance_grades['COM_MAJOR']\n",
        "    dev_stat_weight = dev_stat_weights.get(row.get('DEV_STAT', 'Other'), dev_stat_weights['Other']) * qualitative_importance_grades['DEV_STAT']\n",
        "    ore_weight = ore_weights.get(row.get('ORE', 'Other'), ore_weights['Other']) * qualitative_importance_grades['ORE']\n",
        "    orebody_fm_weight = orebody_fm_weights.get(row.get('OREBODY_FM', 'Other'), orebody_fm_weights['Other']) * qualitative_importance_grades['OREBODY_FM']\n",
        "    work_type_weight = work_type_weights.get(row.get('WORK_TYPE', 'Other'), work_type_weights['Other']) * qualitative_importance_grades['WORK_TYPE']\n",
        "    model_weight = model_weights.get(row.get('MODEL', 'Other'), model_weights['Other']) * qualitative_importance_grades['MODEL']\n",
        "    hrock_unit_weight = hrock_unit_weights.get(row.get('HROCK_UNIT', 'Other'), hrock_unit_weights['Other']) * qualitative_importance_grades['HROCK_UNIT']\n",
        "    hrock_type_weight = hrock_type_weights.get(row.get('HROCK_TYPE', 'Other'), hrock_type_weights['Other']) * qualitative_importance_grades['HROCK_TYPE']\n",
        "    arock_unit_weight = arock_unit_weights.get(row.get('AROCK_UNIT', 'Other'), arock_unit_weights['Other']) * qualitative_importance_grades['AROCK_UNIT']\n",
        "    arock_type_weight = arock_type_weights.get(row.get('AROCK_TYPE', 'Other'), arock_type_weights['Other']) * qualitative_importance_grades['AROCK_TYPE']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = (\n",
        "        com_major_weight +\n",
        "        dev_stat_weight +\n",
        "        ore_weight +\n",
        "        orebody_fm_weight +\n",
        "        work_type_weight +\n",
        "        model_weight +\n",
        "        hrock_unit_weight +\n",
        "        hrock_type_weight +\n",
        "        arock_unit_weight +\n",
        "        arock_type_weight\n",
        "    )\n",
        "\n",
        "    # Ensure weight is rounded to 4 decimal places and non-negative\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_deposits_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each deposit...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_deposit_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only the 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_deposits_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Uranium_and_Vanadium_Deposits_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEJ1WpZtzc4h"
      },
      "source": [
        "## Transportation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaeb5H_ezc4h"
      },
      "source": [
        "### Airports\n",
        "**Dataset: US Airports -** https://hub.arcgis.com/datasets/Aviation::us-airports/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtuGdzjVzc4h",
        "outputId": "04fe6bbe-45bf-46d6-96b5-5e93c2e19d5f"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "facility_use_code_weights = {\n",
        "    'PR': 0.600,\n",
        "    'PU': 0.400\n",
        "}\n",
        "\n",
        "ownership_type_code_weights = {\n",
        "    'PR': 0.600,\n",
        "    'PU': 0.250,\n",
        "    'Other': 0.150  # Other includes MR, MA, MN, CG\n",
        "}\n",
        "\n",
        "region_code_weights = {\n",
        "    'AGL': 0.250,\n",
        "    'ASW': 0.250,\n",
        "    'ASO': 0.250,\n",
        "    'Other': 0.250  # Balance other regions\n",
        "}\n",
        "\n",
        "site_type_code_weights = {\n",
        "    'A': 0.600,\n",
        "    'H': 0.250,\n",
        "    'Other': 0.150  # Other includes C, U, G, B\n",
        "}\n",
        "\n",
        "arpt_status_weights = {\n",
        "    'O': 0.800,\n",
        "    'CI': 0.100,\n",
        "    'CP': 0.100\n",
        "}\n",
        "\n",
        "direction_code_weights = {\n",
        "    'N': 0.111,\n",
        "    'NE': 0.111,\n",
        "    'E': 0.111,\n",
        "    'SE': 0.111,\n",
        "    'S': 0.111,\n",
        "    'SW': 0.111,\n",
        "    'W': 0.111,\n",
        "    'NW': 0.111,\n",
        "    'Other': 0.111  # Distribute evenly among all directions\n",
        "}\n",
        "\n",
        "fuel_types_weights = {\n",
        "    '100LL': 0.600,\n",
        "    'A': 0.400,\n",
        "    'Other': 0.000  # Assume no additional importance for other types not mentioned\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'FACILITY_USE_CODE': 0.150,\n",
        "    'OWNERSHIP_TYPE_CODE': 0.150,\n",
        "    'REGION_CODE': 0.100,\n",
        "    'SITE_TYPE_CODE': 0.150,\n",
        "    'ARPT_STATUS': 0.150,\n",
        "    'DIRECTION_CODE': 0.150,\n",
        "    'FUEL_TYPES': 0.150\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'ELEV': 0.200,\n",
        "    'BASED_SINGLE_ENG': 0.150,\n",
        "    'BASED_MULTI_ENG': 0.100,\n",
        "    'BASED_JET_ENG': 0.150,\n",
        "    'BASED_HEL': 0.100,\n",
        "    'BASED_GLIDERS': 0.050,\n",
        "    'BASED_MIL_ACFT': 0.150,\n",
        "    'BASED_ULTRALGT_ACFT': 0.050,\n",
        "    'COMMERCIAL_OPS': 0.250,\n",
        "    'AIR_TAXI_OPS': 0.150,\n",
        "    'LOCAL_OPS': 0.200,\n",
        "    'ITNRNT_OPS': 0.200,\n",
        "    'MIL_ACFT_OPS': 0.150\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_airport_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    facility_use_weight = facility_use_code_weights.get(row['FACILITY_USE_CODE'], 0.0) * qualitative_importance_grades['FACILITY_USE_CODE']\n",
        "    ownership_type_weight = ownership_type_code_weights.get(row['OWNERSHIP_TYPE_CODE'], ownership_type_code_weights['Other']) * qualitative_importance_grades['OWNERSHIP_TYPE_CODE']\n",
        "    region_code_weight = region_code_weights.get(row['REGION_CODE'], region_code_weights['Other']) * qualitative_importance_grades['REGION_CODE']\n",
        "    site_type_weight = site_type_code_weights.get(row['SITE_TYPE_CODE'], site_type_code_weights['Other']) * qualitative_importance_grades['SITE_TYPE_CODE']\n",
        "    arpt_status_weight = arpt_status_weights.get(row['ARPT_STATUS'], 0.0) * qualitative_importance_grades['ARPT_STATUS']\n",
        "    direction_code_weight = direction_code_weights.get(row['DIRECTION_CODE'], direction_code_weights['Other']) * qualitative_importance_grades['DIRECTION_CODE']\n",
        "    fuel_types_weight = fuel_types_weights.get(row['FUEL_TYPES'], fuel_types_weights['Other']) * qualitative_importance_grades['FUEL_TYPES']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    elev_weight = (row['ELEV'] / max(1, np.nanmax([row.get('ELEV', 0)]))) * quantitative_weights['ELEV'] if row['ELEV'] > 0 else 0\n",
        "    single_eng_weight = (row['BASED_SINGLE_ENG'] / max(1, np.nanmax([row.get('BASED_SINGLE_ENG', 0)]))) * quantitative_weights['BASED_SINGLE_ENG'] if row['BASED_SINGLE_ENG'] > 0 else 0\n",
        "    multi_eng_weight = (row['BASED_MULTI_ENG'] / max(1, np.nanmax([row.get('BASED_MULTI_ENG', 0)]))) * quantitative_weights['BASED_MULTI_ENG'] if row['BASED_MULTI_ENG'] > 0 else 0\n",
        "    jet_eng_weight = (row['BASED_JET_ENG'] / max(1, np.nanmax([row.get('BASED_JET_ENG', 0)]))) * quantitative_weights['BASED_JET_ENG'] if row['BASED_JET_ENG'] > 0 else 0\n",
        "    hel_weight = (row['BASED_HEL'] / max(1, np.nanmax([row.get('BASED_HEL', 0)]))) * quantitative_weights['BASED_HEL'] if row['BASED_HEL'] > 0 else 0\n",
        "    gliders_weight = (row['BASED_GLIDERS'] / max(1, np.nanmax([row.get('BASED_GLIDERS', 0)]))) * quantitative_weights['BASED_GLIDERS'] if row['BASED_GLIDERS'] > 0 else 0\n",
        "    mil_acft_weight = (row['BASED_MIL_ACFT'] / max(1, np.nanmax([row.get('BASED_MIL_ACFT', 0)]))) * quantitative_weights['BASED_MIL_ACFT'] if row['BASED_MIL_ACFT'] > 0 else 0\n",
        "    ultralgt_acft_weight = (row['BASED_ULTRALGT_ACFT'] / max(1, np.nanmax([row.get('BASED_ULTRALGT_ACFT', 0)]))) * quantitative_weights['BASED_ULTRALGT_ACFT'] if row['BASED_ULTRALGT_ACFT'] > 0 else 0\n",
        "    commercial_ops_weight = (row['COMMERCIAL_OPS'] / max(1, np.nanmax([row.get('COMMERCIAL_OPS', 0)]))) * quantitative_weights['COMMERCIAL_OPS'] if row['COMMERCIAL_OPS'] > 0 else 0\n",
        "    air_taxi_ops_weight = (row['AIR_TAXI_OPS'] / max(1, np.nanmax([row.get('AIR_TAXI_OPS', 0)]))) * quantitative_weights['AIR_TAXI_OPS'] if row['AIR_TAXI_OPS'] > 0 else 0\n",
        "    local_ops_weight = (row['LOCAL_OPS'] / max(1, np.nanmax([row.get('LOCAL_OPS', 0)]))) * quantitative_weights['LOCAL_OPS'] if row['LOCAL_OPS'] > 0 else 0\n",
        "    itnrnt_ops_weight = (row['ITNRNT_OPS'] / max(1, np.nanmax([row.get('ITNRNT_OPS', 0)]))) * quantitative_weights['ITNRNT_OPS'] if row['ITNRNT_OPS'] > 0 else 0\n",
        "    mil_acft_ops_weight = (row['MIL_ACFT_OPS'] / max(1, np.nanmax([row.get('MIL_ACFT_OPS', 0)]))) * quantitative_weights['MIL_ACFT_OPS'] if row['MIL_ACFT_OPS'] > 0 else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        facility_use_weight +\n",
        "        ownership_type_weight +\n",
        "        region_code_weight +\n",
        "        site_type_weight +\n",
        "        arpt_status_weight +\n",
        "        direction_code_weight +\n",
        "        fuel_types_weight +\n",
        "        elev_weight +\n",
        "        single_eng_weight +\n",
        "        multi_eng_weight +\n",
        "        jet_eng_weight +\n",
        "        hel_weight +\n",
        "        gliders_weight +\n",
        "        mil_acft_weight +\n",
        "        ultralgt_acft_weight +\n",
        "        commercial_ops_weight +\n",
        "        air_taxi_ops_weight +\n",
        "        local_ops_weight +\n",
        "        itnrnt_ops_weight +\n",
        "        mil_acft_ops_weight\n",
        "    )\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_airports_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each airport...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_airport_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf[['Weight', 'geometry']]\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_airports_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Aviation_Facilities_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xjpcrevzc4h"
      },
      "source": [
        "### Bridges\n",
        "**Dataset: National Bridge Inventory -** https://hub.arcgis.com/datasets/fedmaps::national-bridge-inventory-3/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1otsYbfVzc4h",
        "outputId": "1261d7c8-37d2-427b-928d-e74a9fdcba76"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "state_code_weights = {\n",
        "    '48': 0.400,  # Texas\n",
        "    '39': 0.250,  # Ohio\n",
        "    '17': 0.200,  # Illinois\n",
        "    '6': 0.150    # California\n",
        "}\n",
        "\n",
        "record_type_weights = {\n",
        "    1: 1.000\n",
        "}\n",
        "\n",
        "route_prefix_weights = {\n",
        "    4: 0.350,  # State Highway\n",
        "    3: 0.300,  # Interstate Highway\n",
        "    5: 0.200,  # U.S. Highway\n",
        "    1: 0.150   # Local Road\n",
        "}\n",
        "\n",
        "service_level_weights = {\n",
        "    1: 0.500,  # Mainline\n",
        "    0: 0.300,  # None\n",
        "    7: 0.200   # Frontage Road\n",
        "}\n",
        "\n",
        "direction_weights = {\n",
        "    0: 0.500,  # Unknown\n",
        "    1: 0.200,  # One-way\n",
        "    2: 0.200   # Two-way\n",
        "}\n",
        "\n",
        "undclrenc_eval_weights = {\n",
        "    'N': 0.400,  # Not applicable\n",
        "    '3': 0.300,  # Fair\n",
        "    '4': 0.300   # Poor\n",
        "}\n",
        "\n",
        "bridge_condition_weights = {\n",
        "    'F': 0.400,  # Fair\n",
        "    'G': 0.400,  # Good\n",
        "    'P': 0.200   # Poor\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'STATE_CODE_001': 0.100,\n",
        "    'RECORD_TYPE_005A': 0.100,\n",
        "    'ROUTE_PREFIX_005B': 0.200,\n",
        "    'SERVICE_LEVEL_005C': 0.150,\n",
        "    'DIRECTION_005E': 0.100,\n",
        "    'UNDCLRENCE_EVAL_069': 0.100,\n",
        "    'BRIDGE_CONDITION': 0.250\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'MIN_VERT_CLR_010': 0.150,\n",
        "    'ADT_029': 0.200,\n",
        "    'YEAR_BUILT_027': 0.150,\n",
        "    'YEAR_ADT_030': 0.100,\n",
        "    'PERCENT_ADT_TRUCK_109': 0.100,\n",
        "    'DESIGN_LOAD_031': 0.150,\n",
        "    'DECK_COND_058': 0.200,\n",
        "    'SUPERSTRUCTURE_COND_059': 0.200,\n",
        "    'SUBSTRUCTURE_COND_060': 0.150,\n",
        "    'CHANNEL_COND_061': 0.150,\n",
        "    'CULVERT_COND_062': 0.150,\n",
        "    'OPERATING_RATING_064': 0.200,\n",
        "    'INVENTORY_RATING_066': 0.150,\n",
        "    'STRUCTURAL_EVAL_067': 0.200,\n",
        "    'DECK_GEOMETRY_EVAL_068': 0.150,\n",
        "    'POSTING_EVAL_070': 0.100,\n",
        "    'WATERWAY_EVAL_071': 0.150,\n",
        "    'APPR_ROAD_EVAL_072': 0.150,\n",
        "    'DATE_OF_INSPECT_090': 0.200,\n",
        "    'FRACTURE_LAST_DATE_093A': 0.150,\n",
        "    'UNDWATER_LAST_DATE_093B': 0.150,\n",
        "    'SPEC_LAST_DATE_093C': 0.150,\n",
        "    'LOWEST_RATING': 0.200,\n",
        "    'DECK_AREA': 0.150\n",
        "}\n",
        "\n",
        "# Step 2: Create a function to convert specific columns to numeric\n",
        "\n",
        "def convert_columns_to_numeric(gdf, columns):\n",
        "    for column in columns:\n",
        "        gdf[column] = pd.to_numeric(gdf[column], errors='coerce')\n",
        "    return gdf\n",
        "\n",
        "# Step 3: Calculate the weight for each row\n",
        "\n",
        "def calculate_bridge_weight(row, max_values):\n",
        "    # Calculate qualitative weights\n",
        "    state_code_weight = state_code_weights.get(row['STATE_CODE_001'], 0.0) * qualitative_importance_grades['STATE_CODE_001']\n",
        "    record_type_weight = record_type_weights.get(row['RECORD_TYPE_005A'], 0.0) * qualitative_importance_grades['RECORD_TYPE_005A']\n",
        "    route_prefix_weight = route_prefix_weights.get(row['ROUTE_PREFIX_005B'], 0.0) * qualitative_importance_grades['ROUTE_PREFIX_005B']\n",
        "    service_level_weight = service_level_weights.get(row['SERVICE_LEVEL_005C'], 0.0) * qualitative_importance_grades['SERVICE_LEVEL_005C']\n",
        "    direction_weight = direction_weights.get(row['DIRECTION_005E'], 0.0) * qualitative_importance_grades['DIRECTION_005E']\n",
        "    undclrenc_eval_weight = undclrenc_eval_weights.get(row['UNDCLRENCE_EVAL_069'], 0.0) * qualitative_importance_grades['UNDCLRENCE_EVAL_069']\n",
        "    bridge_condition_weight = bridge_condition_weights.get(row['BRIDGE_CONDITION'], 0.0) * qualitative_importance_grades['BRIDGE_CONDITION']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    quantitative_weight_sum = 0\n",
        "    for key, weight in quantitative_weights.items():\n",
        "        max_value = max_values[key] if key in max_values and max_values[key] > 0 else 1\n",
        "        quantitative_weight_sum += (row.get(key, 0) / max_value) * weight if row.get(key, 0) > 0 else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        state_code_weight +\n",
        "        record_type_weight +\n",
        "        route_prefix_weight +\n",
        "        service_level_weight +\n",
        "        direction_weight +\n",
        "        undclrenc_eval_weight +\n",
        "        bridge_condition_weight +\n",
        "        quantitative_weight_sum\n",
        "    )\n",
        "\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 4: Apply the weights to the dataset\n",
        "\n",
        "def apply_weights_to_bridges_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each bridge...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Convert specific columns to numeric\n",
        "    numeric_columns = [\n",
        "        'MIN_VERT_CLR_010', 'ADT_029', 'YEAR_BUILT_027', 'YEAR_ADT_030', 'PERCENT_ADT_TRUCK_109', 'DESIGN_LOAD_031',\n",
        "        'DECK_COND_058', 'SUPERSTRUCTURE_COND_059', 'SUBSTRUCTURE_COND_060', 'CHANNEL_COND_061', 'CULVERT_COND_062',\n",
        "        'OPERATING_RATING_064', 'INVENTORY_RATING_066', 'STRUCTURAL_EVAL_067', 'DECK_GEOMETRY_EVAL_068', 'POSTING_EVAL_070',\n",
        "        'WATERWAY_EVAL_071', 'APPR_ROAD_EVAL_072', 'DATE_OF_INSPECT_090', 'FRACTURE_LAST_DATE_093A',\n",
        "        'UNDWATER_LAST_DATE_093B', 'SPEC_LAST_DATE_093C', 'LOWEST_RATING', 'DECK_AREA'\n",
        "    ]\n",
        "    gdf = convert_columns_to_numeric(gdf, numeric_columns)\n",
        "\n",
        "    # Get maximum values for each quantitative field\n",
        "    max_values = {col: gdf[col].max() for col in numeric_columns}\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_bridge_weight, axis=1, max_values=max_values)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Drop unnecessary columns, keeping only 'Weight' and 'geometry'\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 5: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_bridges_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/bridge/Bridges_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTQUt9z-zc4i"
      },
      "source": [
        "### Ports\n",
        "**Dataset: Principal Ports -** https://data-usdot.opendata.arcgis.com/datasets/usdot::principal-ports/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4kzRvtjzc4i",
        "outputId": "fabc6030-93fb-4353-fd8f-7438730610ec"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "type_weights = {\n",
        "    'C': 0.600,\n",
        "    'I': 0.300,\n",
        "    'L': 0.100\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'TYPE': 0.500\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'RANK': 0.100,\n",
        "    'TOTAL': 0.200,\n",
        "    'DOMESTIC': 0.150,\n",
        "    'FOREIGN_': 0.200,\n",
        "    'IMPORTS': 0.175,\n",
        "    'EXPORTS': 0.175\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_port_weight(row, max_rank, max_total, max_domestic, max_foreign, max_imports, max_exports):\n",
        "    # Ensure all fields are present and non-null\n",
        "    type_weight = type_weights.get(row['TYPE'], 0.0) * qualitative_importance_grades['TYPE']\n",
        "    rank_weight = (row['RANK'] / max(1, max_rank)) * quantitative_weights['RANK'] if not pd.isna(row['RANK']) else 0\n",
        "    total_weight = (row['TOTAL'] / max(1, max_total)) * quantitative_weights['TOTAL'] if not pd.isna(row['TOTAL']) else 0\n",
        "    domestic_weight = (row['DOMESTIC'] / max(1, max_domestic)) * quantitative_weights['DOMESTIC'] if not pd.isna(row['DOMESTIC']) else 0\n",
        "    foreign_weight = (row['FOREIGN_'] / max(1, max_foreign)) * quantitative_weights['FOREIGN_'] if not pd.isna(row['FOREIGN_']) else 0\n",
        "    imports_weight = (row['IMPORTS'] / max(1, max_imports)) * quantitative_weights['IMPORTS'] if not pd.isna(row['IMPORTS']) else 0\n",
        "    exports_weight = (row['EXPORTS'] / max(1, max_exports)) * quantitative_weights['EXPORTS'] if not pd.isna(row['EXPORTS']) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = type_weight + rank_weight + total_weight + domestic_weight + foreign_weight + imports_weight + exports_weight\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_ports_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each port...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get the maximum values for scaling\n",
        "    max_rank = np.nanmax(gdf['RANK'])\n",
        "    max_total = np.nanmax(gdf['TOTAL'])\n",
        "    max_domestic = np.nanmax(gdf['DOMESTIC'])\n",
        "    max_foreign = np.nanmax(gdf['FOREIGN_'])\n",
        "    max_imports = np.nanmax(gdf['IMPORTS'])\n",
        "    max_exports = np.nanmax(gdf['EXPORTS'])\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_port_weight, axis=1, args=(max_rank, max_total, max_domestic, max_foreign, max_imports, max_exports))\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_ports_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Principal_Ports_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1UcZJt9zc4i"
      },
      "source": [
        "### Railroads\n",
        "**Dataset: North American Rail Lines (NTAD) -** https://hub.arcgis.com/documents/DCP::north-american-rail-lines-ntad/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v04pQqsAzc4i",
        "outputId": "fddbb666-3b15-4e80-c92c-02502f04f572"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "fradistrct_weights = {\n",
        "    99: 0.300,\n",
        "    3: 0.200,\n",
        "    2: 0.150,\n",
        "    'Other': 0.350\n",
        "}\n",
        "\n",
        "rrowner1_weights = {\n",
        "    'UP': 0.350,\n",
        "    'CN': 0.350,\n",
        "    'BNSF': 0.300\n",
        "}\n",
        "\n",
        "passngr_weights = {\n",
        "    'A': 0.400,\n",
        "    'C': 0.250,\n",
        "    'V': 0.200,\n",
        "    'Other': 0.150\n",
        "}\n",
        "\n",
        "net_weights = {\n",
        "    'M': 0.350,\n",
        "    'O': 0.300,\n",
        "    'Y': 0.250,\n",
        "    'Other': 0.100\n",
        "}\n",
        "\n",
        "timezone_weights = {\n",
        "    'C': 0.350,\n",
        "    'E': 0.300,\n",
        "    'P': 0.200,\n",
        "    'Other': 0.150\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'FRADISTRCT': 0.200,\n",
        "    'RROWNER1': 0.300,\n",
        "    'PASSNGR': 0.200,\n",
        "    'NET': 0.150,\n",
        "    'TIMEZONE': 0.150\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'TRACKS': 0.400,\n",
        "    'MILES': 0.350,\n",
        "    'KM': 0.250\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_rail_weight(row, max_tracks, max_miles, max_km):\n",
        "    # Ensure all fields are present and non-null\n",
        "    fradistrct = row.get('FRADISTRCT', 'Other')\n",
        "    rrowner1 = row.get('RROWNER1', 'Other')\n",
        "    passngr = row.get('PASSNGR', 'Other')\n",
        "    net = row.get('NET', 'Other')\n",
        "    timezone = row.get('TIMEZONE', 'Other')\n",
        "    tracks = row.get('TRACKS', 0) if pd.notna(row.get('TRACKS')) else 0\n",
        "    miles = row.get('MILES', 0) if pd.notna(row.get('MILES')) else 0\n",
        "    km = row.get('KM', 0) if pd.notna(row.get('KM')) else 0\n",
        "\n",
        "    # Calculate qualitative weights\n",
        "    fradistrct_weight = fradistrct_weights.get(fradistrct, fradistrct_weights['Other']) * qualitative_importance_grades['FRADISTRCT']\n",
        "    rrowner1_weight = rrowner1_weights.get(rrowner1, 0.0) * qualitative_importance_grades['RROWNER1']\n",
        "    passngr_weight = passngr_weights.get(passngr, passngr_weights['Other']) * qualitative_importance_grades['PASSNGR']\n",
        "    net_weight = net_weights.get(net, net_weights['Other']) * qualitative_importance_grades['NET']\n",
        "    timezone_weight = timezone_weights.get(timezone, timezone_weights['Other']) * qualitative_importance_grades['TIMEZONE']\n",
        "\n",
        "    # Calculate quantitative weights\n",
        "    tracks_weight = (tracks / max_tracks) * quantitative_weights['TRACKS'] if tracks > 0 else 0\n",
        "    miles_weight = (miles / max_miles) * quantitative_weights['MILES'] if miles > 0 else 0\n",
        "    km_weight = (km / max_km) * quantitative_weights['KM'] if km > 0 else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = fradistrct_weight + rrowner1_weight + passngr_weight + net_weight + timezone_weight + tracks_weight + miles_weight + km_weight\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "\n",
        "def apply_weights_to_railroads_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each railroad line...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get the maximum values for scaling\n",
        "    max_tracks = np.nanmax(gdf['TRACKS'])\n",
        "    max_miles = np.nanmax(gdf['MILES'])\n",
        "    max_km = np.nanmax(gdf['KM'])\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_rail_weight, axis=1, args=(max_tracks, max_miles, max_km))\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_railroads_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Rail_Lines_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnrtrJDTFpR5"
      },
      "source": [
        "### Spaceports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fes4bvIFqXk",
        "outputId": "a2582d49-2304-46d2-b49e-b62daa310d0d"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights for the \"License\" property\n",
        "license_weights = {\n",
        "    'FAA': 0.600,\n",
        "    'FAA and Federal': 0.150,\n",
        "    'Federal': 0.150,\n",
        "    'Private Exclusive Use': 0.100\n",
        "}\n",
        "\n",
        "# Define qualitative attribute weights for the \"Launch_Type\" property\n",
        "launch_type_weights = {\n",
        "    'Horizontal': 0.350,\n",
        "    'Vertical': 0.250,\n",
        "    'Vertical and Horizontal': 0.150,\n",
        "    'Not Specified': 0.100,\n",
        "    'Orbital Reentry': 0.050,\n",
        "    'Horizontal and Orbital Reentry': 0.050,\n",
        "    'Horizontal and Vertical': 0.050\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'License': 0.500,\n",
        "    'Launch_Type': 0.500\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_spaceport_weight(row):\n",
        "    # Ensure all fields are present and non-null\n",
        "    license = row.get('License', 'Not Specified') if pd.notna(row.get('License', 'Not Specified')) else 'Not Specified'\n",
        "    launch_type = row.get('Launch_Type', 'Not Specified') if pd.notna(row.get('Launch_Type', 'Not Specified')) else 'Not Specified'\n",
        "\n",
        "    # Calculate qualitative weights\n",
        "    license_weight = license_weights.get(license, 0.0) * qualitative_importance_grades['License']\n",
        "    launch_type_weight = launch_type_weights.get(launch_type, 0.0) * qualitative_importance_grades['Launch_Type']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = license_weight + launch_type_weight\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_spaceports_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each spaceport...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_spaceport_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_spaceports_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Spaceports_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv-YNOv-zc4i"
      },
      "source": [
        "### Roads\n",
        "Transportation - https://hub.arcgis.com/maps/fedmaps::transportation-1/about"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHxXa2Emzc4i"
      },
      "source": [
        "#### Primary Roads\n",
        "**Dataset: Primary_Roads -** https://hub.arcgis.com/datasets/fedmaps::transportation-1/about?layer=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TELv0eMzc4j",
        "outputId": "8de8ad2c-4fbf-4b64-dff0-9fb4ee61c07f"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "pretypeabrv_weights = {\n",
        "    'I-': 0.400,\n",
        "    'US Hwy': 0.300,\n",
        "    'State Rte': 0.150,\n",
        "    'State Hwy': 0.100,\n",
        "    'Others': 0.050\n",
        "}\n",
        "\n",
        "rttyp_weights = {\n",
        "    'I': 0.400,\n",
        "    'M': 0.300,\n",
        "    'U': 0.200,\n",
        "    'S': 0.100,\n",
        "    'Others': 0.050\n",
        "}\n",
        "\n",
        "predirabrv_weights = {\n",
        "    'W': 0.400,\n",
        "    'S': 0.300,\n",
        "    'N': 0.200,\n",
        "    'E': 0.100,\n",
        "    'Others': 0.050\n",
        "}\n",
        "\n",
        "prequal_weights = {\n",
        "    '20.0': 1.000\n",
        "}\n",
        "\n",
        "prequalabrv_weights = {\n",
        "    'Old': 1.000\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'PRETYPEABRV': 0.250,\n",
        "    'RTTYP': 0.250,\n",
        "    'PREDIRABRV': 0.250,\n",
        "    'PREQUAL': 0.125,\n",
        "    'PREQUALABRV': 0.125\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'SUFDIR': 0.333,\n",
        "    'PRETYP': 0.333,\n",
        "    'PREDIR': 0.333\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_road_weight(row, max_sufdir, max_pretyp, max_predir):\n",
        "    # Ensure no missing fields lead to NULL values\n",
        "    pretypeabrv_weight = pretypeabrv_weights.get(row.get('PRETYPEABRV', 'Others'), pretypeabrv_weights['Others']) * qualitative_importance_grades['PRETYPEABRV']\n",
        "    rttyp_weight = rttyp_weights.get(row.get('RTTYP', 'Others'), rttyp_weights['Others']) * qualitative_importance_grades['RTTYP']\n",
        "    predirabrv_weight = predirabrv_weights.get(row.get('PREDIRABRV', 'Others'), predirabrv_weights['Others']) * qualitative_importance_grades['PREDIRABRV']\n",
        "    prequal_weight = prequal_weights.get(row.get('PREQUAL', 0), 0.0) * qualitative_importance_grades['PREQUAL']\n",
        "    prequalabrv_weight = prequalabrv_weights.get(row.get('PREQUALABRV', 0), 0.0) * qualitative_importance_grades['PREQUALABRV']\n",
        "\n",
        "    # Handle quantitative attributes\n",
        "    sufdirt_weight = (row['SUFDIR'] / max_sufdir) * quantitative_weights['SUFDIR'] if row['SUFDIR'] else 0\n",
        "    pretyp_weight = (row['PRETYP'] / max_pretyp) * quantitative_weights['PRETYP'] if row['PRETYP'] else 0\n",
        "    predir_weight = (row['PREDIR'] / max_predir) * quantitative_weights['PREDIR'] if row['PREDIR'] else 0\n",
        "\n",
        "    # Combine all weights\n",
        "    total_weight = (\n",
        "        pretypeabrv_weight +\n",
        "        rttyp_weight +\n",
        "        predirabrv_weight +\n",
        "        prequal_weight +\n",
        "        prequalabrv_weight +\n",
        "        sufdirt_weight +\n",
        "        pretyp_weight +\n",
        "        predir_weight\n",
        "    )\n",
        "\n",
        "    # Ensure weight is a positive float, rounded to 4 digits\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_roads_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each road...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get max values for scaling quantitative attributes\n",
        "    max_sufdir = gdf['SUFDIR'].max() if not gdf['SUFDIR'].isna().all() else 1\n",
        "    max_pretyp = gdf['PRETYP'].max() if not gdf['PRETYP'].isna().all() else 1\n",
        "    max_predir = gdf['PREDIR'].max() if not gdf['PREDIR'].isna().all() else 1\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_road_weight, axis=1, args=(max_sufdir, max_pretyp, max_predir))\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_roads_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Primary_Roads_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6Mpz3Pczc4j"
      },
      "source": [
        "#### Secondary Roads\n",
        "**Dataset: Secondary_Roads_72_1k_scale -** https://hub.arcgis.com/datasets/fedmaps::transportation-1/about?layer=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1FobF2lzc4m"
      },
      "outputs": [],
      "source": [
        "def calculate_road_importance(roads):\n",
        "    importance_scores = []\n",
        "    max_shape_length = max(road['Shape Length'] for road in roads)\n",
        "\n",
        "    # Weights for different Route Type Codes, with 'S' representing secondary roads\n",
        "    route_type_weights = {'S': 0.4, 'M': 0.3, 'U': 0.2, 'C': 0.1, 'O': 0.05, 'I': 0.05}\n",
        "\n",
        "    for road in roads:\n",
        "        # Get the weight for the route type, default to lowest if not found\n",
        "        route_type_score = route_type_weights.get(road['Route Type Code'], 0.05)\n",
        "\n",
        "        # Normalize the shape length to a 0-1 scale and adjust its weight\n",
        "        normalized_length_score = (road['Shape Length'] / max_shape_length) * 0.6\n",
        "\n",
        "        # Calculate total importance score\n",
        "        total_score = route_type_score + normalized_length_score\n",
        "\n",
        "        # Ensure total score does not exceed 1.0 and round to three decimal places\n",
        "        importance_score = round(min(1.0, total_score), 3)\n",
        "        importance_scores.append(importance_score)\n",
        "\n",
        "    return importance_scores\n",
        "\n",
        "# Example usage\n",
        "roads = [\n",
        "    {'Route Type Code': 'S', 'Shape Length': 458.81718699964},\n",
        "    {'Route Type Code': 'M', 'Shape Length': 8888.27937627825},\n",
        "    # Add more entries as needed\n",
        "]\n",
        "\n",
        "importance_scores = calculate_road_importance(roads)\n",
        "print(importance_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMzFNFJxzc4m"
      },
      "source": [
        "## Waste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aXxDl40zc4m"
      },
      "source": [
        "### Solid Waste Landfill Facilities\n",
        "**Dataset: Solid Waste Landfill Facilities -** https://hub.arcgis.com/datasets/155761d340764921ab7fb2e88257bd97_0/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aji1YTHuzc4n",
        "outputId": "f575ad2d-02bb-4ce9-fd28-bac2321209de"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "facility_type_weights = {\n",
        "    'Solid Waste': 1.000,\n",
        "}\n",
        "\n",
        "facility_status_weights = {\n",
        "    'Inactive': 0.200,\n",
        "    'Active': 0.300,\n",
        "    'Pre-Authorized': 0.200,\n",
        "    'Closed, No Gw Monitoring': 0.150,\n",
        "    'Nfa, No Further Action': 0.100,\n",
        "    'Other Statuses': 0.050,\n",
        "}\n",
        "\n",
        "ownership_weights = {\n",
        "    'Private': 0.400,\n",
        "    'County': 0.300,\n",
        "    'Municipal': 0.200,\n",
        "    'Other Ownership Types': 0.100,\n",
        "}\n",
        "\n",
        "district_weights = {\n",
        "    'SED (Southeast District)': 0.250,\n",
        "    'NED (Northeast District)': 0.250,\n",
        "    'NWD (Northwest District)': 0.200,\n",
        "    'CD (Central District)': 0.200,\n",
        "    'Other Districts': 0.100,\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'FACILITY_TYPE': 0.400,\n",
        "    'FACILITY_STATUS': 0.300,\n",
        "    'OWNERSHIP': 0.200,\n",
        "    'DISTRICT': 0.100,\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_facility_weight(row):\n",
        "    # Ensure no field is missing and replace with zero if necessary\n",
        "    facility_type_weight = facility_type_weights.get(row.get('FACILITY_TYPE', ''), 0.0) * qualitative_importance_grades['FACILITY_TYPE']\n",
        "    facility_status_weight = facility_status_weights.get(row.get('FACILITY_STATUS', ''), 0.0) * qualitative_importance_grades['FACILITY_STATUS']\n",
        "    ownership_weight = ownership_weights.get(row.get('OWNERSHIP', ''), 0.0) * qualitative_importance_grades['OWNERSHIP']\n",
        "    district_weight = district_weights.get(row.get('DISTRICT', ''), 0.0) * qualitative_importance_grades['DISTRICT']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = (\n",
        "        facility_type_weight +\n",
        "        facility_status_weight +\n",
        "        ownership_weight +\n",
        "        district_weight\n",
        "    )\n",
        "\n",
        "    # Ensure weight is positive and rounded to 4 decimal places\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "\n",
        "def apply_weights_to_facilities_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each facility...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_facility_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Drop unnecessary columns, keeping only 'Weight' and 'geometry'\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_facilities_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Solid_Waste_Facilities_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbsW3p8ezc4n"
      },
      "source": [
        "## Water"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lt7CurUzc4n"
      },
      "source": [
        "### Aquifers\n",
        "**Dataset: Aquifers -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::aquifers/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvFws18Wzc4n",
        "outputId": "5148c50a-9749-4660-cba0-7af57310bca4"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "rock_name_weights = {\n",
        "    \"Other rocks\": 0.200,\n",
        "    \"Unconsolidated sand and gravel aquifers\": 0.250,\n",
        "    \"Carbonate-rock aquifers\": 0.200,\n",
        "    \"Semiconsolidated sand aquifers\": 0.150,\n",
        "    \"Igneous and metamorphic-rock aquifers\": 0.100,\n",
        "    \"Sandstone aquifers\": 0.050,\n",
        "    \"Sandstone and carbonate-rock aquifers\": 0.050\n",
        "}\n",
        "\n",
        "rock_type_weights = {\n",
        "    999: 0.200,\n",
        "    100: 0.250,\n",
        "    400: 0.200,\n",
        "    200: 0.150,\n",
        "    600: 0.100,\n",
        "    300: 0.050,\n",
        "    500: 0.050\n",
        "}\n",
        "\n",
        "aq_code_weights = {\n",
        "    999: 0.400,\n",
        "    201: 0.300,\n",
        "    609: 0.200,\n",
        "    104: 0.100\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'ROCK_NAME': 0.350,\n",
        "    'ROCK_TYPE': 0.350,\n",
        "    'AQ_CODE': 0.300\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "\n",
        "def calculate_aquifer_weight(row):\n",
        "    # Ensure all fields are present and valid\n",
        "    rock_name = row.get('ROCK_NAME', \"Other rocks\") if pd.notna(row.get('ROCK_NAME')) else \"Other rocks\"\n",
        "    rock_type = row.get('ROCK_TYPE', 999) if pd.notna(row.get('ROCK_TYPE')) else 999\n",
        "    aq_code = row.get('AQ_CODE', 999) if pd.notna(row.get('AQ_CODE')) else 999\n",
        "\n",
        "    # Calculate qualitative weights\n",
        "    rock_name_weight = rock_name_weights.get(rock_name, 0.0) * qualitative_importance_grades['ROCK_NAME']\n",
        "    rock_type_weight = rock_type_weights.get(rock_type, 0.0) * qualitative_importance_grades['ROCK_TYPE']\n",
        "    aq_code_weight = aq_code_weights.get(aq_code, 0.0) * qualitative_importance_grades['AQ_CODE']\n",
        "\n",
        "    # Combine qualitative weights\n",
        "    total_weight = rock_name_weight + rock_type_weight + aq_code_weight\n",
        "\n",
        "    # Ensure weight is positive and round to 4 decimal places\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "\n",
        "def apply_weights_to_aquifers_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each aquifer...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_aquifer_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    # Keep only 'Weight' and 'geometry' columns\n",
        "    gdf = gdf[['Weight', 'geometry']]\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_aquifers_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Aquifers_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k4nBvJ5zc4n"
      },
      "source": [
        "### Dams\n",
        "**Dataset: National Inventory of Dams (NID) -** https://hifld-geoplatform.hub.arcgis.com/apps/geoplatform::national-inventory-of-dams-nid/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml3PKAhbzc4n",
        "outputId": "0583d667-06aa-46ed-b18a-b47e564507de"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "owner_type_weights = {\n",
        "    'Private': 0.300,\n",
        "    'Local Government': 0.200,\n",
        "    'State': 0.150,\n",
        "    'Federal': 0.150,\n",
        "    'Other': 0.200\n",
        "}\n",
        "\n",
        "purpose_weights = {\n",
        "    'Recreation': 0.300,\n",
        "    'Flood Risk Reduction': 0.250,\n",
        "    'Fire Protection, Stock, Or Small Fish Pond': 0.150,\n",
        "    'Other': 0.150,\n",
        "    'Other Types': 0.150\n",
        "}\n",
        "\n",
        "spillway_type_weights = {\n",
        "    'Uncontrolled': 0.500,\n",
        "    'Controlled': 0.300,\n",
        "    'None': 0.200\n",
        "}\n",
        "\n",
        "hazard_potential_weights = {\n",
        "    'Low': 0.400,\n",
        "    'High': 0.300,\n",
        "    'Significant': 0.200,\n",
        "    'Not Available / Undetermined': 0.100\n",
        "}\n",
        "\n",
        "dam_type_weights = {\n",
        "    'Earth': 0.500,\n",
        "    'Gravity': 0.200,\n",
        "    'Concrete': 0.200,\n",
        "    'Other': 0.100\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'PRIMARY_OWNER_TYPE': 0.200,\n",
        "    'PRIMARY_PURPOSE': 0.250,\n",
        "    'SPILLWAY_TYPE': 0.150,\n",
        "    'HAZARD_POTENTIAL': 0.300,\n",
        "    'PRIMARY_DAM_TYPE': 0.100\n",
        "}\n",
        "\n",
        "# Define quantitative attribute weights\n",
        "quantitative_weights = {\n",
        "    'DAM_HEIGHT': 0.200,\n",
        "    'NID_STORAGE': 0.150,\n",
        "    'MAX_STORAGE': 0.150,\n",
        "    'NUMBER_ASSOCIATED_STRUCTURES': 0.100,\n",
        "    'DISTANCE': 0.100,\n",
        "    'HYDRAULIC_HEIGHT': 0.100,\n",
        "    'STRUCTURAL_HEIGHT': 0.100,\n",
        "    'DAM_LENGTH': 0.150,\n",
        "    'DAM_VOLUME': 0.200,\n",
        "    'NORMAL_STORAGE': 0.150,\n",
        "    'SURFACE_AREA': 0.150,\n",
        "    'DRAINAGE_AREA': 0.150,\n",
        "    'MAX_DISCHARGE': 0.200,\n",
        "    'SPILLWAY_WIDTH': 0.100,\n",
        "    'NUMBER_OF_LOCKS': 0.100,\n",
        "    'LENGTH_OF_LOCKS': 0.100,\n",
        "    'WIDTH_OF_LOCKS': 0.100\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row, ignoring non-existent or null values\n",
        "\n",
        "def calculate_dam_weight(row, max_values):\n",
        "    # Calculate qualitative weights, ignoring missing or null values\n",
        "    owner_type_weight = owner_type_weights.get(row.get('PRIMARY_OWNER_TYPE'), 0.0) * qualitative_importance_grades['PRIMARY_OWNER_TYPE']\n",
        "    purpose_weight = purpose_weights.get(row.get('PRIMARY_PURPOSE', 'Other'), purpose_weights['Other']) * qualitative_importance_grades['PRIMARY_PURPOSE']\n",
        "    spillway_type_weight = spillway_type_weights.get(row.get('SPILLWAY_TYPE', 'None'), 0.0) * qualitative_importance_grades['SPILLWAY_TYPE']\n",
        "    hazard_potential_weight = hazard_potential_weights.get(row.get('HAZARD_POTENTIAL', 'Not Available / Undetermined'), 0.0) * qualitative_importance_grades['HAZARD_POTENTIAL']\n",
        "    dam_type_weight = dam_type_weights.get(row.get('PRIMARY_DAM_TYPE', 'Other'), 0.0) * qualitative_importance_grades['PRIMARY_DAM_TYPE']\n",
        "\n",
        "    # Calculate quantitative weights, ignoring missing values\n",
        "    dam_height_weight = (row['DAM_HEIGHT'] / max(1, max_values['DAM_HEIGHT'])) * quantitative_weights['DAM_HEIGHT'] if not pd.isna(row['DAM_HEIGHT']) else 0\n",
        "    nid_storage_weight = (row['NID_STORAGE'] / max(1, max_values['NID_STORAGE'])) * quantitative_weights['NID_STORAGE'] if not pd.isna(row['NID_STORAGE']) else 0\n",
        "    max_storage_weight = (row['MAX_STORAGE'] / max(1, max_values['MAX_STORAGE'])) * quantitative_weights['MAX_STORAGE'] if not pd.isna(row['MAX_STORAGE']) else 0\n",
        "    num_structures_weight = (row['NUMBER_ASSOCIATED_STRUCTURES'] / max(1, max_values['NUMBER_ASSOCIATED_STRUCTURES'])) * quantitative_weights['NUMBER_ASSOCIATED_STRUCTURES'] if not pd.isna(row['NUMBER_ASSOCIATED_STRUCTURES']) else 0\n",
        "    distance_weight = (row['DISTANCE'] / max(1, max_values['DISTANCE'])) * quantitative_weights['DISTANCE'] if not pd.isna(row['DISTANCE']) else 0\n",
        "    hydraulic_height_weight = (row['HYDRAULIC_HEIGHT'] / max(1, max_values['HYDRAULIC_HEIGHT'])) * quantitative_weights['HYDRAULIC_HEIGHT'] if not pd.isna(row['HYDRAULIC_HEIGHT']) else 0\n",
        "    structural_height_weight = (row['STRUCTURAL_HEIGHT'] / max(1, max_values['STRUCTURAL_HEIGHT'])) * quantitative_weights['STRUCTURAL_HEIGHT'] if not pd.isna(row['STRUCTURAL_HEIGHT']) else 0\n",
        "    dam_length_weight = (row['DAM_LENGTH'] / max(1, max_values['DAM_LENGTH'])) * quantitative_weights['DAM_LENGTH'] if not pd.isna(row['DAM_LENGTH']) else 0\n",
        "    normal_storage_weight = (row['NORMAL_STORAGE'] / max(1, max_values['NORMAL_STORAGE'])) * quantitative_weights['NORMAL_STORAGE'] if not pd.isna(row['NORMAL_STORAGE']) else 0\n",
        "    surface_area_weight = (row['SURFACE_AREA'] / max(1, max_values['SURFACE_AREA'])) * quantitative_weights['SURFACE_AREA'] if not pd.isna(row['SURFACE_AREA']) else 0\n",
        "    drainage_area_weight = (row['DRAINAGE_AREA'] / max(1, max_values['DRAINAGE_AREA'])) * quantitative_weights['DRAINAGE_AREA'] if not pd.isna(row['DRAINAGE_AREA']) else 0\n",
        "    max_discharge_weight = (row['MAX_DISCHARGE'] / max(1, max_values['MAX_DISCHARGE'])) * quantitative_weights['MAX_DISCHARGE'] if not pd.isna(row['MAX_DISCHARGE']) else 0\n",
        "    spillway_width_weight = (row['SPILLWAY_WIDTH'] / max(1, max_values['SPILLWAY_WIDTH'])) * quantitative_weights['SPILLWAY_WIDTH'] if not pd.isna(row['SPILLWAY_WIDTH']) else 0\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = (\n",
        "        owner_type_weight +\n",
        "        purpose_weight +\n",
        "        spillway_type_weight +\n",
        "        hazard_potential_weight +\n",
        "        dam_type_weight +\n",
        "        dam_height_weight +\n",
        "        nid_storage_weight +\n",
        "        max_storage_weight +\n",
        "        num_structures_weight +\n",
        "        distance_weight +\n",
        "        hydraulic_height_weight +\n",
        "        structural_height_weight +\n",
        "        dam_length_weight +\n",
        "        normal_storage_weight +\n",
        "        surface_area_weight +\n",
        "        drainage_area_weight +\n",
        "        max_discharge_weight +\n",
        "        spillway_width_weight\n",
        "    )\n",
        "\n",
        "    # Ensure weight is positive, rounded to 4 decimal places, and no negative values\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "\n",
        "def apply_weights_to_dams_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each dam...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Precompute the max values for all the relevant fields to avoid repeated computation\n",
        "    max_values = {\n",
        "        'DAM_HEIGHT': gdf['DAM_HEIGHT'].max(skipna=True),\n",
        "        'NID_STORAGE': gdf['NID_STORAGE'].max(skipna=True),\n",
        "        'MAX_STORAGE': gdf['MAX_STORAGE'].max(skipna=True),\n",
        "        'NUMBER_ASSOCIATED_STRUCTURES': gdf['NUMBER_ASSOCIATED_STRUCTURES'].max(skipna=True),\n",
        "        'DISTANCE': gdf['DISTANCE'].max(skipna=True),\n",
        "        'HYDRAULIC_HEIGHT': gdf['HYDRAULIC_HEIGHT'].max(skipna=True),\n",
        "        'STRUCTURAL_HEIGHT': gdf['STRUCTURAL_HEIGHT'].max(skipna=True),\n",
        "        'DAM_LENGTH': gdf['DAM_LENGTH'].max(skipna=True),\n",
        "        'NORMAL_STORAGE': gdf['NORMAL_STORAGE'].max(skipna=True),\n",
        "        'SURFACE_AREA': gdf['SURFACE_AREA'].max(skipna=True),\n",
        "        'DRAINAGE_AREA': gdf['DRAINAGE_AREA'].max(skipna=True),\n",
        "        'MAX_DISCHARGE': gdf['MAX_DISCHARGE'].max(skipna=True),\n",
        "        'SPILLWAY_WIDTH': gdf['SPILLWAY_WIDTH'].max(skipna=True)\n",
        "    }\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_dam_weight, axis=1, max_values=max_values)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "\n",
        "def process_and_save_weighted_geojson(input_path, output_path):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Size in MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_dams_dataset(gdf)\n",
        "\n",
        "    # Save the weighted dataset with only 'Weight' and 'geometry' fields\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted[['Weight', 'geometry']].to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Size in MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/Dams_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted/Dams_Weighted.geojson\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(os.path.dirname(output_dir), exist_ok=True)\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS9zP2Kyzc4o"
      },
      "source": [
        "### USACE Owned & Operated Reservoirs\n",
        "**Dataset: US Army Corps of Engineers (USACE) Owned and Operated Reservoirs -** https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::us-army-corps-of-engineers-usace-owned-and-operated-reservoirs/about"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVk4-Y-_zc4o",
        "outputId": "91bff88d-b140-41f7-a266-8a95204e440a"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Define the weight schemas\n",
        "\n",
        "# Define qualitative attribute weights\n",
        "district_weights = {\n",
        "    'Huntington District': 0.200,\n",
        "    'Mobile District': 0.200,\n",
        "    'Tulsa District': 0.180,\n",
        "    'New England District': 0.160,\n",
        "    'Omaha District': 0.160\n",
        "}\n",
        "\n",
        "dist_sym_weights = {\n",
        "    'LRH': 0.200,\n",
        "    'SAM': 0.200,\n",
        "    'SWT': 0.180,\n",
        "    'NAE': 0.160,\n",
        "    'NWO': 0.160\n",
        "}\n",
        "\n",
        "division_weights = {\n",
        "    'Southwestern Division': 0.250,\n",
        "    'Great Lakes and Ohio River Division': 0.240,\n",
        "    'Northwestern Division': 0.200,\n",
        "    'North Atlantic Division': 0.160,\n",
        "    'South Atlantic Division': 0.150,\n",
        "    'South Pacific Division': 0.120,\n",
        "    'Mississippi Valley Division': 0.080\n",
        "}\n",
        "\n",
        "dry_weights = {\n",
        "    'No': 0.800,\n",
        "    'Yes': 0.150,\n",
        "    'NO': 0.050\n",
        "}\n",
        "\n",
        "# Define overall importance grades for each qualitative property\n",
        "qualitative_importance_grades = {\n",
        "    'DISTRICT': 0.250,\n",
        "    'DIST_SYM': 0.250,\n",
        "    'DIVISION': 0.300,\n",
        "    'DRY': 0.200\n",
        "}\n",
        "\n",
        "# Step 2: Calculate the weight for each row\n",
        "def calculate_reservoir_weight(row):\n",
        "    # Calculate qualitative weights\n",
        "    district_weight = district_weights.get(row.get('DISTRICT', ''), 0.0) * qualitative_importance_grades['DISTRICT']\n",
        "    dist_sym_weight = dist_sym_weights.get(row.get('DIST_SYM', ''), 0.0) * qualitative_importance_grades['DIST_SYM']\n",
        "    division_weight = division_weights.get(row.get('DIVISION', ''), 0.0) * qualitative_importance_grades['DIVISION']\n",
        "    dry_weight = dry_weights.get(row.get('DRY', ''), 0.0) * qualitative_importance_grades['DRY']\n",
        "\n",
        "    # Combine qualitative and quantitative weights\n",
        "    total_weight = district_weight + dist_sym_weight + division_weight + dry_weight\n",
        "\n",
        "    # Ensure the weight is positive, and round to 4 digits\n",
        "    return round(max(0.0, total_weight), 4)\n",
        "\n",
        "# Step 3: Apply the weights to the dataset\n",
        "def apply_weights_to_reservoirs_dataset(gdf):\n",
        "    print(\"Starting to calculate weights for each reservoir...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the weight calculation function to each row\n",
        "    gdf['Weight'] = gdf.apply(calculate_reservoir_weight, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Weights calculated. Time elapsed: {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    return gdf[['Weight', 'geometry']]  # Keep only Weight and geometry columns\n",
        "\n",
        "# Step 4: Process and save the dataset with weights\n",
        "def process_and_save_weighted_geojson(input_path, output_dir):\n",
        "    # Load the dataset\n",
        "    print(f\"Loading dataset from {input_path}...\")\n",
        "    gdf = gpd.read_file(input_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Measure initial file size\n",
        "    initial_size = os.path.getsize(input_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Initial file size: {initial_size:.2f} MB\")\n",
        "\n",
        "    # Apply weights\n",
        "    gdf_weighted = apply_weights_to_reservoirs_dataset(gdf)\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_name = os.path.basename(input_path).replace(\".geojson\", \"_weighted_cleaned.geojson\")\n",
        "    output_path = os.path.join(output_dir, output_file_name)\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save the weighted dataset\n",
        "    print(f\"Saving weighted dataset to {output_path}...\")\n",
        "    gdf_weighted.to_file(output_path, driver='GeoJSON')\n",
        "\n",
        "    # Measure final file size\n",
        "    final_size = os.path.getsize(output_path) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"Final file size: {final_size:.2f} MB\")\n",
        "\n",
        "    # Calculate file size reduction\n",
        "    reduction = initial_size - final_size\n",
        "    reduction_percentage = (reduction / initial_size) * 100 if initial_size > 0 else 0\n",
        "    print(f\"File size reduced by: {reduction:.2f} MB ({reduction_percentage:.2f}% reduction).\")\n",
        "\n",
        "# Define paths\n",
        "input_path = \"/geoJSON/cleaned/USACE_Reservoirs_Cleaned.geojson\"\n",
        "output_dir = \"/geoJSON/cleaned_weighted\"\n",
        "\n",
        "# Process and save the dataset\n",
        "process_and_save_weighted_geojson(input_path, output_dir)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "K0oLRRxOzc33",
        "_BPO7zU7zc35",
        "G7CqMViUzc37",
        "KLntwLl8zc38",
        "4uNc_pWizc38",
        "r1OlKvdazc3-",
        "VuMue-Bvzc3-",
        "xRbDJP5uzc3_",
        "Qa5rQgEBzc4A",
        "ptDu_Hoezc4A",
        "LSmXsuHHzc4M",
        "6g2TJSmSzc4R",
        "D7St1aRkzc4S",
        "Jw5Sh9VSzc4T",
        "gDMTdLuzzc4T",
        "RdkDwS9gzc4T",
        "znUWgvVUzc4U",
        "cRj-nY0Tzc4U",
        "6wKtF1rHzc4V",
        "_D7pOKC0zc4W",
        "W4evf5k0zc4X",
        "8cc0A3Tjzc4X",
        "f3BSmKKIzc4X",
        "mDHv99OWzc4Y",
        "3Jfs6i0szc4Y",
        "zrzwE585zc4Z",
        "HUbZ1KkBzc4Z",
        "uOCLhgrlzc4a",
        "INjJDpdPzc4a",
        "y-5NlwGlXEkt",
        "wImgMYYMXJPt",
        "EaFNUM8Kzc4a",
        "vao1X7qBzc4b",
        "IjibIWNizc4b",
        "K0lHMgx4zc4b",
        "Trj5UTvwzc4c",
        "lgfXMMdgzc4c",
        "Bc53a_gGzc4c",
        "XMMl_D9rzc4d",
        "cnoYyhQkzc4d",
        "GhhU6Y7Kzc4e",
        "8XIIF2C_zc4e",
        "e4osvBjHzc4e",
        "Cu7LIG_bzc4f",
        "Wlpf11ouzc4f",
        "lLJdWuwjzc4g",
        "I7vs_nEAzc4g",
        "xHdlKLaHzc4g",
        "zaeb5H_ezc4h",
        "7Xjpcrevzc4h",
        "PTQUt9z-zc4i",
        "X1UcZJt9zc4i",
        "GnrtrJDTFpR5",
        "M6Mpz3Pczc4j",
        "-lt7CurUzc4n",
        "-k4nBvJ5zc4n"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
